{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonathanwu/anaconda3/envs/ECE5970/lib/python3.5/site-packages/IPython/core/interactiveshell.py:2698: DtypeWarning: Columns (85,86,91,92,101,102,103,104,105,106,107,108,109,456,820,1398,1643,1645,1651,1652,1881,1882,1883,1884,1885,1886,1887,1888,1889,1890,1891) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n",
      "/Users/jonathanwu/anaconda3/envs/ECE5970/lib/python3.5/site-packages/ipykernel_launcher.py:23: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "/Users/jonathanwu/anaconda3/envs/ECE5970/lib/python3.5/site-packages/ipykernel_launcher.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Importing the dataset with library pandas\n",
    "dataset = pd.read_csv('TADPOLE_InputData.csv')\n",
    "labels_train = pd.read_csv('TADPOLE_TargetData_train.csv')\n",
    "labels_test = pd.read_csv('TADPOLE_TargetData_test.csv')\n",
    "\n",
    "# Tally the data types of all data columns, and then separate them according to dtype.\n",
    "dtypeCounts = dataset.dtypes.value_counts(); # Count the number of columns for each data type. Turns out to be only 'float64' and 'object'.\n",
    "numDataset = dataset.select_dtypes(include=['float'])\n",
    "objDataset = dataset.select_dtypes(include=['object'])\n",
    "\n",
    "# Unfortunately some numerical columns would be grouped into objDataset by the operations above, since they contain entries with non-numerical symbols such as in '>1300'.\n",
    "# Basically the strategy is to convert these columns to floats by extracting only numbers, including int and float.\n",
    "columnsTofix = ['ABETA_UPENNBIOMK9_04_19_17','TAU_UPENNBIOMK9_04_19_17','PTAU_UPENNBIOMK9_04_19_17','COMMENT_UPENNBIOMK9_04_19_17']\n",
    "for column in columnsTofix:\n",
    "    strIdx = np.where(objDataset[column].apply(type).values == str)[0] # Find all str type entries in each column, which may or may not contain non-numerical characters such as '<' or '>'.\n",
    "    for row in strIdx: # iterate through each column\n",
    "        objDataset.set_value(row,column,float(re.search('\\d+.?\\d*',objDataset[column].values[row]).group(0))) # get rid of '<' '>' if there is any and convert to float  \n",
    "    objDataset[column]=pd.to_numeric(objDataset[column],errors='coerce') # cast each column to float type\n",
    "numDataset = pd.concat([numDataset,objDataset[columnsTofix]], axis = 1) # Move this section of data to numDataset\n",
    "objDataset = objDataset.drop(columnsTofix,axis=1) # Remove this section from objDataset\n",
    "\n",
    "\n",
    "badColumns = ['update_stamp_UCSFFSL_02_01_16_UCSFFSL51ALL_08_01_16',\n",
    "    'update_stamp_UCSFFSX_11_02_15_UCSFFSX51_08_01_16',\n",
    "    'update_stamp_UCBERKELEYAV45_10_17_16',\n",
    "    'update_stamp_DTIROI_04_30_14']\n",
    "objDataset = objDataset.drop(badColumns,axis=1) # Remove this section from objDataset\n",
    "\n",
    "''' The badColumns are strange:  \n",
    "    They have mixed data types, some entries (e.g. 33:37.0) seem to be timing, \n",
    "    but the others (e.g. 428e+4), seem to be float with the same repeating values.\n",
    "    What do they mean? Before understanding them, I leave them out of objDataset.\n",
    "'''\n",
    "\n",
    "''' The folloing codes were used to detect the problematic columns (columnsTofix and badColumns)mentioned above\n",
    "for column in objDataset:\n",
    "    if sum(objDataset[column].apply(type) == float) -  sum(pd.isnull(objDataset[column])) > 0: # returns true if there is at least one entry that is float but not 'NaN'. 'NaN' are excluded since they are float, but not really numerical.\n",
    "        print(column)\n",
    "'''\n",
    "\n",
    "# Separate date and time columns from objDataset (which was desgined to contain only categorical data)\n",
    "for column in objDataset: # variable 'column' is a string\n",
    "    if objDataset[column].str.match('[0-9]+/[0-9]+/[0-9]+').sum() > 0: # returns true if this column contains at least one string that matches date format.\n",
    "        objDataset[column] = pd.to_datetime(objDataset[column],format=\"%m/%d/%y\",errors='coerce') # convert string to date\n",
    "dateDataset = objDataset.select_dtypes(include=['datetime64']) # select dates from objDataset for variable dateDataset\n",
    "objDataset = objDataset.drop(dateDataset.columns,axis=1) # Remove dates from objDataset\n",
    "\n",
    "nonnanNumDataset = numDataset.count() # returns the count of non-NaN entries for each column in numDataset, since not only we want to impute, we want to know how many we impute, especially for columns with very sparse initial data.\n",
    "# Histogram of non-NaN percentage of columns\n",
    "temp = numDataset.count()/8717\n",
    "temp.hist(bins=50)\n",
    "numX = numDataset.iloc[:,:].values # converting to ndarray makes it easier to work with sklearn imputer, see below\n",
    "\n",
    "# Imputing missing data in numX\n",
    "from sklearn.preprocessing import Imputer\n",
    "imp = Imputer(missing_values='NaN', strategy='most_frequent', axis=0, verbose=0, copy=True) # imput numerical columns\n",
    "numX2 = imp.fit_transform(numX)\n",
    "nan_index = []\n",
    "for i in range(numX.shape[1]): #find all columns that are invalid\n",
    "    curr_vect = numX[:,i]\n",
    "    curr_sum = np.nansum(curr_vect)\n",
    "    if curr_sum==0 :\n",
    "        nan_index.append(i)\n",
    "        \n",
    "nan_index2=[]\n",
    "for i in range(numX2.shape[1]):#find all columns that are false positives\n",
    "    curr_vect = numX2[:,i]\n",
    "    curr_sum = np.nansum(curr_vect)\n",
    "    if curr_sum==0 :\n",
    "        nan_index2.append(i)\n",
    "    \n",
    "for i in range(len(nan_index2)): #remove false positives from invalid columns\n",
    "    nan_index.remove(nan_index2[i]) \n",
    "\n",
    "num_cat_names = [] #get new category after all the dropped cols\n",
    "for i in range(len(numDataset.columns)):\n",
    "    if i not in nan_index:\n",
    "        num_cat_names.append(numDataset.columns[i])\n",
    "\n",
    "cat_name_to_num_dict = {} #map category name to corresponding column\n",
    "for i in range(len(num_cat_names)):\n",
    "    cat_name_to_num_dict[num_cat_names[i]] = numX2[:,i]\n",
    "\n",
    "\n",
    "# Encoding categorical data, missing data, aka 'NaN', will be encoded to zero(s).\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "objDataset = objDataset.fillna('Null') # replace all 'NaN's with 'Null', which has not appeared in the entire objDataset.\n",
    "catX = objDataset.iloc[:,:].values # Convert categorical data from DataFrame to ndarray for ease of operation\n",
    "catX_pre_ohe = objDataset.iloc[:,:].values\n",
    "lab = LabelEncoder();\n",
    "for i in range(catX.shape[1]):\n",
    "    catX[:,i] = lab.fit_transform(catX[:,i])\n",
    "    catX_pre_ohe[:,i] = lab.fit_transform(catX_pre_ohe[:,i])\n",
    "catX_pre_ohe = np.asarray(catX_pre_ohe,dtype=np.float64)\n",
    "ohe = OneHotEncoder(categorical_features = 'all') # To binary data, for example, for three categories 1, 2, 3, it is translated to 00, 01, 10.\n",
    "catX = ohe.fit_transform(catX).toarray()\n",
    "\n",
    "\n",
    "cat_to_ohe_dict = {}\n",
    "last_index = 0\n",
    "for i in range(catX_pre_ohe.shape[1]):\n",
    "    col_span = catX_pre_ohe[:,i].max()\n",
    "    splice_index = int(last_index + col_span + 1)\n",
    "    cat_to_ohe_dict[i] = catX[:,last_index:splice_index]\n",
    "    last_index = splice_index\n",
    "\n",
    "cat_name_to_cat_dict = {}\n",
    "cat_names = objDataset.columns\n",
    "for i in range(len(cat_names)):\n",
    "    cat_name_to_cat_dict[cat_names[i]]=cat_to_ohe_dict[i] #map category name to np arrays\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Work on labels_train: convert dates to number of days relative to the initial date, and then bind to the rest of data and do imputation\n",
    "idxPTIDTable_train = pd.concat([pd.DataFrame(labels_train.index.values),labels_train['PTID_Key']],axis=1)\n",
    "dic_train = idxPTIDTable_train.groupby('PTID_Key').groups\n",
    "dateY_train_raw = pd.to_datetime(labels_train['Date'],format=\"%m/%d/%y\",errors='coerce')\n",
    "dateY_train = np.zeros(labels_train['Date'].size)\n",
    "dateY_train[:] = np.nan\n",
    "for key, value in dic_train.items():\n",
    "    allDates = dateY_train_raw[value] # Get all dates in column i about patient with ID 'key'\n",
    "    if not allDates.empty: # There is at least one row about this patient\n",
    "        firstDate = allDates.min()\n",
    "        if not pd.isnull(firstDate): # There is at least one valid date in this column, which is the earliest date of this column\n",
    "            for j in range(allDates.size):\n",
    "                dateY_train[value[j]] = (allDates[value[j]]-firstDate).days\n",
    "y_train = np.concatenate((dateY_train[:,None],labels_train.iloc[:,1:].values),axis=1)\n",
    "imp = imp.fit(y_train)\n",
    "y_train = imp.transform(y_train)\n",
    "\n",
    "\n",
    "\n",
    "# Work on labels_test: convert dates to number of days relative to the initial date, and then bind to the rest of data and do imputation\n",
    "idxPTIDTable_test = pd.concat([pd.DataFrame(labels_test.index.values),labels_test['PTID_Key']],axis=1)\n",
    "dic_test = idxPTIDTable_test.groupby('PTID_Key').groups\n",
    "dateY_test_raw = pd.to_datetime(labels_test['Date'],format=\"%Y-%m-%d\",errors='coerce')\n",
    "dateY_test = np.zeros(dateY_test_raw.size)\n",
    "dateY_test[:] = np.nan\n",
    "for key, value in dic_test.items():\n",
    "    allDates = dateY_test_raw[value] # Get all dates in column i about patient with ID 'key'\n",
    "    if not allDates.empty: # There is at least one row about this patient\n",
    "        firstDate = allDates.min()\n",
    "        if not pd.isnull(firstDate): # There is at least one valid date in this column, which is the earliest date of this column\n",
    "            for j in range(allDates.size):\n",
    "                dateY_test[value[j]] = (allDates[value[j]]-firstDate).days\n",
    "y_test = np.concatenate((dateY_test[:,None],labels_test.iloc[:,1:].values),axis=1)\n",
    "imp = imp.fit(y_test)\n",
    "y_test = imp.transform(y_test)\n",
    "yAttributes = labels_test.columns.values\n",
    "yAttributes = np.delete(yAttributes, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for y train set\n",
    "y_train_sub_dates=y_train[:,1:] #take out absolute days feature\n",
    "cat_name_to_y_train = {}#create dicitonary mapping categories to the train feature column\n",
    "index=0\n",
    "for cat in labels_train.columns[1:]:\n",
    "    cat_name_to_y_train[cat]=y_train_sub_dates[:,index]\n",
    "    index+=1\n",
    "\n",
    "#for y test set\n",
    "y_test_sub_dates = y_test[:,1:]#take out absolute days feature\n",
    "cat_name_to_y_test = {}#create dictionary mapping categories to test feature column\n",
    "index=0\n",
    "for cat in labels_test.columns[1:]:\n",
    "    cat_name_to_y_test[cat]=y_test_sub_dates[:,index]\n",
    "    index+=1\n",
    "\n",
    "#for y valid set\n",
    "labels_valid = pd.read_csv('TADPOLE_PredictTargetData_valid.csv')#read in csv using pandas frame\n",
    "y_valid_ids = labels_valid.iloc[:,1].values #loads y_valid_ids with PTIDs in set\n",
    "\n",
    "cat_name_to_y_valid = {}#create dictionary mapping categories to y valid features\n",
    "index = 0\n",
    "for cat in labels_valid.columns[1:]:\n",
    "    cat_name_to_y_valid[cat] = labels_valid.iloc[:, (index+1)].values\n",
    "    index+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input S must be dictionary w/ 'PTID_Key' as one of keys\n",
    "#maps patient ids tomost recent (last appearing index in matrix )\n",
    "# returns dictionary with\n",
    "# key: PT_ID         val: last index row index\n",
    "def get_last_patient_visit(S):\n",
    "    hist_dict={}\n",
    "    for pt_id in S['PTID_Key']: #get indexes of last patient visit per patient id\n",
    "        pt_id_appearances = np.where(S['PTID_Key']==pt_id)\n",
    "        hist_dict[int(pt_id)] = np.max(pt_id_appearances)\n",
    "    return hist_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#condense dataset into 1 visit per patient (last visit for each patient)\n",
    "def make_last_visit(dataset, last_visit_dict):\n",
    "    try:\n",
    "        last_visit_dataset = np.zeros( (len(last_visit_dict.keys()), dataset.shape[1]) )\n",
    "    except:\n",
    "        last_visit_dataset = np.zeros( (len(last_visit_dict.keys()), ))\n",
    "    ind = 0\n",
    "    for k in last_visit_dict.keys():\n",
    "        last_visit_index = last_visit_dict[k]\n",
    "        last_visit_dataset[ind] = dataset[last_visit_index]\n",
    "        ind+=1\n",
    "    return last_visit_dataset\n",
    "#get last visits for respective sets\n",
    "X_patient_last_visit = get_last_patient_visit(cat_name_to_num_dict)\n",
    "y_train_last_visit = get_last_patient_visit(cat_name_to_y_train)\n",
    "y_test_last_visit = get_last_patient_visit(cat_name_to_y_test)\n",
    "y_valid_last_visit = get_last_patient_visit(cat_name_to_y_valid)\n",
    "\n",
    "last_visit_y_train = np.zeros((len(y_train_last_visit.keys()),y_train_sub_dates.shape[1]))\n",
    "lv_y_train = make_last_visit(y_train_sub_dates, y_train_last_visit)\n",
    "last_visit_y_test = make_last_visit(y_test_sub_dates, y_test_last_visit)\n",
    "last_visit_y_valid = make_last_visit(y_valid_ids, y_valid_last_visit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#condense to one visit / id\n",
    "index_y = 0\n",
    "for key in y_train_last_visit.keys():\n",
    "    last_visit_index = y_train_last_visit[key]\n",
    "    last_visit_y_train[index_y] = y_train_sub_dates[last_visit_index]\n",
    "    index_y+=1\n",
    "    \n",
    "index_X=0\n",
    "for key in X_patient_last_visit.keys():\n",
    "    last_visit_index = X_patient_last_visit[key]\n",
    "    index_X+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X and y are np arrays, X/y_last is dict of PTID -> last row index in their respective datasets\n",
    "#converts X so that it has same # of patients as y and each row corresponds to the same patient\n",
    "def modify_X_to_y(X, X_last, y_last): \n",
    "    y_PT_ID = y_last.keys()\n",
    "    modified_X = np.zeros((len(y_PT_ID), X.shape[1]))\n",
    "    X_PTID_Index = list(X[:,0]) #index here corresponds to vector's index in matrix\n",
    "    index = 0\n",
    "    for pt_id in y_PT_ID:\n",
    "        corr_X_index = X_PTID_Index.index(pt_id) #PT_ID -> X_index = PT_ID-1\n",
    "        modified_X[index] = X[corr_X_index]\n",
    "        index+=1\n",
    "    return modified_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ventricles_norm is computed as \"Ventricles\" divided by \"ICV\"\n",
    "#DXCHANGE = {1, 7, 9} encodes healthy control, DXCHANGE = {2, 4, 8} encodes MCI, and \n",
    "#DXCHANGE = {3, 5, 6} encodes Alzheimer's diagnosis.\n",
    "def DXCHANGE_to_diagnosis(DXCHANGE_col):\n",
    "    expanded_DXCHANGE = np.zeros((len(DXCHANGE_col), 3))\n",
    "    healthy = set([1,7,9])\n",
    "    mci = set([2,4,8])\n",
    "    ad = set([3,5,6])\n",
    "    for i in range(len(DXCHANGE_col)):\n",
    "        if int(DXCHANGE_col[i]) in healthy:\n",
    "            new_row = np.array([1,0,0])\n",
    "            expanded_DXCHANGE[i] = new_row\n",
    "        elif int(DXCHANGE_col[i]) in mci:\n",
    "            new_row = np.array([0,1,0])\n",
    "            expanded_DXCHANGE[i] = new_row\n",
    "        elif int(DXCHANGE_col[i]) in ad:\n",
    "            new_row = np.array([0,0,1])\n",
    "            expanded_DXCHANGE[i] = new_row\n",
    "    return expanded_DXCHANGE\n",
    "\n",
    "#converts the one hot encoding to a 0, 1, or 2 depending on diagnosis\n",
    "def diagnosis_to_classes(y_diagnosis_cols):\n",
    "    condensed_classes = np.zeros((y_diagnosis_cols.shape[0],))\n",
    "    for i in range(len(condensed_classes)):\n",
    "        check_equivalence_healthy = y_diagnosis_cols[i] == np.array([1,0,0])\n",
    "        check_equivalence_mci = y_diagnosis_cols[i] == np.array([0,1,0])\n",
    "        check_equivalence_ad = y_diagnosis_cols[i] == np.array([0,0,1])\n",
    "        if check_equivalence_healthy.all() == True:\n",
    "            condensed_classes[i] = 0\n",
    "        elif check_equivalence_mci.all() == True:\n",
    "            condensed_classes[i] = 1\n",
    "        elif check_equivalence_ad.all() == True:\n",
    "            condensed_classes[i] = 2        \n",
    "    return condensed_classes\n",
    "#converts one hot encoding of diagnosis to one column w/ classes 0-healthy, 1-MCI, 2-AD\n",
    "def classes_to_diagnosis(y_diagnosis_col):\n",
    "    condensed_classes = np.zeros((y_diagnosis_col.shape[0], 3))\n",
    "    for i in range(len(condensed_classes)):\n",
    "        if y_diagnosis_col[i] == 0:\n",
    "            condensed_classes[i] = np.array([1,0,0])\n",
    "        elif y_diagnosis_col[i] == 1:\n",
    "            condensed_classes[i] = np.array([0,1,0])\n",
    "        elif y_diagnosis_col[i] == 2:\n",
    "            condensed_classes[i] = np.array([0,0,1])     \n",
    "    return condensed_classes\n",
    "                                                            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dxchange = cat_name_to_num_dict['DXCHANGE']\n",
    "exp_DXCHANGE = DXCHANGE_to_diagnosis(dxchange)\n",
    "y_train_condensed_classes = diagnosis_to_classes(y_train_sub_dates[:,1:4])\n",
    "\n",
    "y_train_lv_condensed_classes = make_last_visit(y_train_condensed_classes, y_train_last_visit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_mod_train = modify_X_to_y(numX2, X_patient_last_visit, y_train_last_visit)#X input w/ corresponding patient ids as train set\n",
    "X_mod_test = modify_X_to_y(numX2, X_patient_last_visit, y_test_last_visit)#X input w/ corresponding pt ids as test\n",
    "X_mod_valid = modify_X_to_y(numX2, X_patient_last_visit, y_valid_last_visit)#X input w/ corresponding pt ids as valid\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Regression Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=100000.0, class_weight=None, dual=False,\n",
       "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "          multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
       "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "X = X_mod_train[:,1:] \n",
    "y_mmse = lv_y_train[:,6]\n",
    "\n",
    "MMSE_lin_reg = LogisticRegression(C=1e5)\n",
    "MMSE_lin_reg.fit(X,y_mmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "MMSE_test_predict = MMSE_lin_reg.predict(X_mod_test[:,1:])\n",
    "MMSE_valid_predict = MMSE_lin_reg.predict(X_mod_valid[:,1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Diagnosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=100000.0, class_weight=None, dual=False,\n",
       "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "          multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
       "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg_diagnosis = LogisticRegression(C=1e5)\n",
    "X = X_mod_train[:,1:] \n",
    "y = y_train_lv_condensed_classes\n",
    "logreg_diagnosis.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get probablilty of each diagnoses\n",
    "diagnosis_predict_test = logreg_diagnosis.predict(X_mod_test[:,1:])\n",
    "expanded_cols_test = classes_to_diagnosis(diagnosis_predict_test)\n",
    "expanded_prob_test = logreg_diagnosis.predict_proba(X_mod_test[:,1:])\n",
    "\n",
    "#get regular diagnosis\n",
    "diagnosis_predict_valid = logreg_diagnosis.predict(X_mod_valid[:,1:])\n",
    "expanded_cols_valid = classes_to_diagnosis(diagnosis_predict_valid)\n",
    "expanded_prob_valid = logreg_diagnosis.predict_proba(X_mod_valid[:,1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ADAS13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=100000.0, class_weight=None, dual=False,\n",
       "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "          multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
       "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg_adas13 = LogisticRegression(C=1e5)\n",
    "X = X_mod_train[:,1:] \n",
    "y = lv_y_train[:,4]\n",
    "logreg_adas13.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_adas13_test=logreg_adas13.predict(X_mod_test[:,1:])\n",
    "predicted_adas13_valid=logreg_adas13.predict(X_mod_valid[:,1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ventricle Norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=True)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "X = X_mod_train[:,1:] \n",
    "y = lv_y_train[:,5]\n",
    "\n",
    "vn_lin_reg = LinearRegression(normalize=True)\n",
    "vn_lin_reg.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "vn_predict_test = vn_lin_reg.predict(X_mod_test[:,1:])\n",
    "vn_predict_valid = vn_lin_reg.predict(X_mod_valid[:,1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_col_index(category):\n",
    "    return num_cat_names.index(category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output of vn lin regression model has some negative values, so i replace the negative values with vn calculated from\n",
    "#historic patient data\n",
    "def floor_vn_values(vn_prediction, X_corr):\n",
    "    mod_vn_predict = np.zeros((len(vn_prediction), ))\n",
    "    vent_index = get_col_index('Ventricles')\n",
    "    icv_index = get_col_index('ICV')\n",
    "    for i in range(len(X_corr[:,0])):\n",
    "        curr_PTID = X_corr[:,0][i]\n",
    "        \n",
    "        if(vn_prediction[i] < 0):\n",
    "            #print 'flooring'\n",
    "            corr_PTID_row = list(X_corr[:,0]).index(curr_PTID) #gets row index for value to fix\n",
    "            mod_vn_predict[i] = (X_corr[corr_PTID_row][vent_index]/X_corr[corr_PTID_row][icv_index])\n",
    "            #print('floored', mod_vn_predict[i])\n",
    "        else:\n",
    "            mod_vn_predict[i] = vn_prediction[i]\n",
    "    return mod_vn_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "floored_vn_predict_test = floor_vn_values(vn_predict_test, X_mod_test)\n",
    "floored_vn_predict_valid = floor_vn_values(vn_predict_valid, X_mod_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Prediction CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatenate all the test values together\n",
    "predicted_y_test = np.column_stack( (X_mod_test[:,0], expanded_cols_test, predicted_adas13_test, \n",
    "                                     floored_vn_predict_test, MMSE_test_predict) )\n",
    "#test values with diagnosis cols as probabilities\n",
    "predicted_y_test_prob = np.column_stack( (X_mod_test[:,0], expanded_prob_test, predicted_adas13_test, \n",
    "                                     floored_vn_predict_test, MMSE_test_predict) )\n",
    "#concatenate all the valid values together\n",
    "predicted_y_valid = np.column_stack( (X_mod_valid[:,0], expanded_cols_valid, predicted_adas13_valid, \n",
    "                                      floored_vn_predict_valid, MMSE_valid_predict) )\n",
    "#valid values w/ diagnosis cols as probabilities\n",
    "predicted_y_valid_prob = np.column_stack( (X_mod_valid[:,0], expanded_prob_valid, predicted_adas13_valid, \n",
    "                                      floored_vn_predict_valid, MMSE_valid_predict) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "filled_out_y_test = np.zeros( (len(labels_test.iloc[:,1].values), 7) )\n",
    "filled_out_y_test_prob = np.zeros( (len(labels_test.iloc[:,1].values), 7) )\n",
    "\n",
    "filled_out_y_valid = np.zeros( (len(labels_valid.iloc[:,1].values), 7) )\n",
    "filled_out_y_valid_prob = np.zeros( (len(labels_valid.iloc[:,1].values), 7) )\n",
    "\n",
    "#fill in all previous visits with the predicted visit\n",
    "index = 0\n",
    "predicted_y_test_ids = list(predicted_y_test[:,0])\n",
    "for test_ids in labels_test.iloc[:,1].values:\n",
    "    predicted_y_test_row = predicted_y_test_ids.index(test_ids)\n",
    "    filled_out_y_test[index] = predicted_y_test[predicted_y_test_row]\n",
    "    filled_out_y_test_prob[index] = predicted_y_test_prob[predicted_y_test_row]\n",
    "    index+=1\n",
    "    \n",
    "index = 0\n",
    "predicted_y_valid_ids = list(predicted_y_valid[:,0])\n",
    "for valid_ids in labels_valid.iloc[:,1].values:\n",
    "    predicted_y_valid_row = predicted_y_valid_ids.index(valid_ids)\n",
    "    filled_out_y_valid[index] = predicted_y_valid[predicted_y_valid_row]\n",
    "    filled_out_y_valid_prob[index] = predicted_y_valid_prob[predicted_y_valid_row]\n",
    "    index+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatenate dates onto data\n",
    "filled_out_y_test = np.column_stack( (labels_test['Date'], filled_out_y_test) )\n",
    "filled_out_y_valid = np.column_stack( (labels_valid['Date'], filled_out_y_valid) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write predictions to a dataframe and export df to a csv\n",
    "baseline_test_data_df = pd.DataFrame(data=filled_out_y_test, columns=labels_test.columns)\n",
    "baseline_validation_data_df = pd.DataFrame(data=filled_out_y_valid, columns=labels_valid.columns)\n",
    "output_folder = 'results/'\n",
    "output_test = 'TADPOLE_baseline_test.csv'\n",
    "output_validation = 'TADPOLE_baseline_valid.csv'\n",
    "baseline_test_data_df.to_csv( (output_folder+output_test), index=False )\n",
    "baseline_validation_data_df.to_csv( (output_folder+output_validation), index=False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATAPREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TIANYU's CODE\n",
    "def PTLump(data,key):\n",
    "    ptid_roster = data['PTID_Key'].astype(int)\n",
    "    ptid_roster = ptid_roster.dropna(how='all') \n",
    "    ptid_roster = ptid_roster.unique()\n",
    "    tbl = pd.concat([pd.DataFrame(data['PTID_Key'].index.values),data['PTID_Key'].astype(int)],axis=1)\n",
    "    dic = tbl.groupby('PTID_Key').groups\n",
    "    rowIdx = []\n",
    "    for i in ptid_roster:\n",
    "        allDates = data[key].iloc[dic[i]]\n",
    "        print(i,allDates.idxmax())\n",
    "        rowIdx.append(allDates.idxmax()) \n",
    "    reducX = data.iloc[rowIdx,:]\n",
    "    reducX = reducX.reset_index(drop=True)\n",
    "    return reducX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonathanwu/anaconda3/envs/ECE5970/lib/python3.5/site-packages/IPython/core/interactiveshell.py:2698: DtypeWarning: Columns (85,86,91,92,101,102,103,104,105,106,107,108,109,456,820,1398,1643,1645,1651,1652,1881,1882,1883,1884,1885,1886,1887,1888,1889,1890,1891) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Importing the dataset with library pandas\n",
    "dataset = pd.read_csv('TADPOLE_InputData.csv')\n",
    "labels_train = pd.read_csv('TADPOLE_TargetData_train.csv')\n",
    "labels_test = pd.read_csv('TADPOLE_TargetData_test.csv')\n",
    "target = pd.read_csv('TADPOLE_PredictTargetData_valid.csv')\n",
    "\n",
    "# Drop meaningless rows and columns. A good practice is to drop rows before columns.\n",
    "dataset = dataset[~np.isnan(dataset['PTID_Key'])] # Drop patients with no ID, since they cannot be used for learning or prediction. [] slices the rows in dataframe.\n",
    "dataset = dataset.dropna(axis=1, how='all') # Drop empty columns in dataset \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop all columns with more than 80% of its entries as NaN\n",
    "nan_threshold = 0.8 * dataset.shape[0]\n",
    "dataset_stats = dataset.isnull().sum()\n",
    "col_indexes_to_drop = []\n",
    "for i in range(len(dataset_stats)):\n",
    "    if dataset_stats[i] > nan_threshold:\n",
    "        col_indexes_to_drop.append(i)\n",
    "dataset.drop(dataset.columns[col_indexes_to_drop],axis=1,inplace=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonathanwu/anaconda3/envs/ECE5970/lib/python3.5/site-packages/pandas/core/indexing.py:194: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n",
      "/Users/jonathanwu/anaconda3/envs/ECE5970/lib/python3.5/site-packages/ipykernel_launcher.py:76: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "##TIANYU's CODE\n",
    "#Sort all datasets by ascending patient ID.\n",
    "dataset = dataset.sort_values('PTID_Key')\n",
    "labels_train = labels_train.sort_values('PTID_Key')\n",
    "labels_test = labels_test.sort_values('PTID_Key')\n",
    "target = target.sort_values('PTID_Key')\n",
    "\n",
    "# Reindex rows\n",
    "dataset = dataset.reset_index(drop=True)\n",
    "labels_train = labels_train.reset_index(drop=True)\n",
    "labels_test = labels_test.reset_index(drop=True)\n",
    "target = target.reset_index(drop=True)\n",
    "\n",
    "# These columns are time stamps that do not carry particular meaning, so dropped for now.\n",
    "badColumns = ['update_stamp_UCSFFSL_02_01_16_UCSFFSL51ALL_08_01_16',\n",
    "    'update_stamp_UCSFFSX_11_02_15_UCSFFSX51_08_01_16']#,\n",
    "    #'update_stamp_UCBERKELEYAV45_10_17_16']#,\n",
    "    #'update_stamp_DTIROI_04_30_14']\n",
    "dataset = dataset.drop(badColumns,axis=1) # Remove this section from objDataset\n",
    "\n",
    "# Unfortunately, some numerical columns contain non-numerical character such as '>' in  '>1300'.\n",
    "# The strategy is to convert these columns to floats by extracting only numbers. For example, '>1300' goes to 1300\n",
    "columnsObjToNum = ['ABETA_UPENNBIOMK9_04_19_17','TAU_UPENNBIOMK9_04_19_17','PTAU_UPENNBIOMK9_04_19_17']#,'COMMENT_UPENNBIOMK9_04_19_17']\n",
    "#columnsObjToNum = ['PTAU_UPENNBIOMK9_04_19_17']#,'COMMENT_UPENNBIOMK9_04_19_17']\n",
    "\n",
    "for column in columnsObjToNum:\n",
    "    colIdx = dataset.columns.get_loc(column)\n",
    "    rowIdx = np.where(dataset[column].apply(type).values == str)[0] # Find all str type elements in each column, which may or may not contain non-numerical characters such as '<' or '>'.\n",
    "    for row in rowIdx: # iterate through each row of string type element in the column\n",
    "        dataset.iloc[row,colIdx] = float(re.sub(\"[^0-9.]\",\"\",dataset[column].values[row])) # Find the float/int number in the string, and cast to float type. \n",
    "dataset[columnsObjToNum] = dataset[columnsObjToNum].astype(float) # cast each column to float type\n",
    "\n",
    "# Convert date columns to date format in dataset, since they are currently imported as object columns\n",
    "for column in dataset: # variable 'column' is a string\n",
    "    if dataset[column].dtype == 'object' and dataset[column].str.match('[0-9]+/[0-9]+/[0-9]+').sum() > 0: # returns true if this column contains at least one string that matches date format.\n",
    "        dataset[column] = pd.to_datetime(dataset[column],format=\"%m/%d/%y\",errors='coerce') # convert string to date\n",
    "\n",
    "\n",
    "# Some numerical columns contain only one/few possible values, which are more likely to be categorical than numerical features.\n",
    "# As a result, such columns are converted to objective dtype. e.g. some column with only -4 and nan.\n",
    "columnsNumToCat = []\n",
    "for column in dataset:\n",
    "    psbVal = dataset[column].unique()\n",
    "    if psbVal.dtype == 'float64' and psbVal.size <= 20:\n",
    "        columnsNumToCat.append(column)\n",
    "        validRowIdx = dataset[column].notnull()\n",
    "        dataset[column].loc[validRowIdx] = dataset[column].loc[validRowIdx].astype(str)\n",
    "#dataset[columnsNumToCat] = dataset[columnsNumToCat].astype(object) # cast to object columns\n",
    "\n",
    "# Tally the data types of all data columns, and then separate them according to dtype.\n",
    "dtypeCounts = dataset.dtypes.value_counts(); # Count the number of columns for each data type. Turns out to be only 'float64' and 'object'.\n",
    "numDataset = dataset.select_dtypes(include=['float'])\n",
    "objDataset = dataset.select_dtypes(include=['object'])\n",
    "dateDataset = dataset.select_dtypes(include=['datetime64']) # select dates from objDataset for variable dateDataset\n",
    "\n",
    "# Count the number of nan's in each column to get an idea of how sparse each column is. It is very likely to drop sparse columns unless they are highly correlated to results.\n",
    "nonNanCounts_num = numDataset.count() # returns the count of non-NaN entries for each column in numDataset, since not only we want to impute, we want to know how many we impute, especially for columns with very sparse initial data.\n",
    "#temp = numDataset.count()/8715\n",
    "#temp.hist(bins=50)\n",
    "\n",
    "''' The folloing codes were used to detect the problematic columns (columnsTofix and badColumns)mentioned above\n",
    "for column in objDataset:\n",
    "    if sum(objDataset[column].apply(type) == float) -  sum(pd.isnull(objDataset[column])) > 0: # returns true if there is at least one entry that is float but not 'NaN'. 'NaN' are excluded since they are float, but not really numerical.\n",
    "        print(column)\n",
    "'''\n",
    "\n",
    "# Imputing missing data in numDataset\n",
    "from sklearn.preprocessing import Imputer\n",
    "imp = Imputer(missing_values = 'NaN', strategy = 'mean',axis = 0) # impute numerical columns\n",
    "imp = imp.fit(numDataset)\n",
    "numX = imp.transform(numDataset) # Extract data from numData as numX in dtype ndarray\n",
    "#numAttributes = numDataset.columns.values # Names of columns in numX. Executed after imputation since all NaN columns are dropped.\n",
    "\n",
    "# Encoding categorical data\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "objDataset.loc[-1,:] = np.repeat(np.nan,objDataset.shape[1]) # Append one NaN to the end of each column so that NaN must be a class for each column.\n",
    "objDataset = objDataset.fillna(value=' ') # LabelEncoder does not work with NaN, so NaN is converted to a space ' ', which is always sorted as the first class by LabelEncoder.\n",
    "le = LabelEncoder()\n",
    "catClasses = np.array([]) # initiate an empty list of attribute names\n",
    "catAttributes = np.array([]) # initiate an empty list of attribute names\n",
    "for i in range(objDataset.shape[1]):\n",
    "    objDataset.iloc[:,i] = le.fit_transform(objDataset.iloc[:,i]) # encode column i\n",
    "    classes = le.classes_ # All the labels in column i, including ' ', which was translated from NaN\n",
    "    classes[0] = 'NaN' # Replace ' ' with the attribute of the column\n",
    "    catClasses = np.append(catClasses,classes) # Append column attribute followed by all its labels to catAttribute\n",
    "    catAttributes = np.append(catAttributes, np.repeat(objDataset.columns.values[i],classes.size))\n",
    "    \n",
    "tups = [catAttributes,catClasses]\n",
    "attrTups = list(zip(*tups)) \n",
    "enc = OneHotEncoder(categorical_features = 'all') # Based on numerical categories in objDataset, encode it to one in n-class features. e.g. 0 => 0, 0, 0, ...; 1 => 0, 1, 0, ...; 2 => 0, 0, 1, 0, ...\n",
    "catDataset = pd.DataFrame(enc.fit_transform(objDataset).toarray()) # Dtype: csr_matrix => numpy array => dataframe\n",
    "catDataset = catDataset.iloc[:-1,:] # Remove the last line full of NaN added earlier\n",
    "objDataset = objDataset.iloc[:-1,:] # Remove the last line full of NaN added earlier\n",
    "multiIdx = pd.MultiIndex.from_tuples(attrTups, names = ['Attribute','Class']) # Construct a two-level column names in the format of catDataset[Attribute][Class]\n",
    "catDataset.columns = multiIdx \n",
    "\n",
    "# Convert dates to numerical, as relative days since the first date in the column for the same patient.\n",
    "# First build a dictionary between PTID_Key (patient ID) and line indices. THis will also be useful for lumping data.\n",
    "idxPTIDTable = pd.concat([pd.DataFrame(np.array(range(0,numDataset.shape[0]))),numDataset['PTID_Key']],axis=1)\n",
    "dic = idxPTIDTable.groupby('PTID_Key').groups\n",
    "\n",
    "# Each entry of dateX is the number of days since the first date of the same patient in the same column.\n",
    "dateX = np.zeros(dateDataset.shape)\n",
    "dateX[:] = np.nan\n",
    "for key, value in dic.items():\n",
    "    for i in range(dateDataset.shape[1]): # iterate through each date column\n",
    "        allDates = dateDataset.iloc[value,i] # Get all dates in column i about patient with ID 'key'\n",
    "        firstDate = allDates.min()\n",
    "        validDateIdx = np.where(~pd.isnull(allDates))[0] # indices of all non-NaT dates\n",
    "        # if not pd.isnull(firstDate): # There is at least one valid date in this column, which is the earliest date of this column\n",
    "        for j in range(len(validDateIdx)):\n",
    "            dateX[value[j],i] = (allDates[value[j]]-firstDate).days\n",
    "\n",
    "# Repacking datasets for ease of inspection\n",
    "dateDataset = pd.DataFrame(dateX, columns = dateDataset.columns.values) \n",
    "numDataset = pd.DataFrame(numX, columns = numDataset.columns.values) # numX is after imputation.\n",
    "Data = pd.concat([numDataset,catDataset], axis=1) #numDataset is imputed, catDataset does not need imputation since NaN is a class, dateDataset is not imputed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TIANYU's CODE\n",
    "#Work on labels_train: convert dates to number of days relative to the initial date, and then bind to the rest of data and do imputation\n",
    "idxPTIDTable_train = pd.concat([pd.DataFrame(labels_train.index.values),labels_train['PTID_Key']],axis=1)\n",
    "dic_train = idxPTIDTable_train.groupby('PTID_Key').groups\n",
    "dateY_train_raw = pd.to_datetime(labels_train['Date'],format=\"%m/%d/%y\",errors='coerce')\n",
    "dateY_train = np.zeros(labels_train['Date'].size)\n",
    "dateY_train[:] = np.nan\n",
    "for key, value in dic_train.items():\n",
    "    allDates = dateY_train_raw[value] # Get all dates in column i about patient with ID 'key'\n",
    "    firstDate = allDates.min()\n",
    "    validDateIdx = np.where(~pd.isnull(allDates))[0]\n",
    "    for j in range(len(validDateIdx)):\n",
    "        dateY_train[value[j]] = (allDates[value[j]]-firstDate).days\n",
    "labels_train['Date'] = dateY_train\n",
    "\n",
    "#On Training labels: Impute missing data based on most frequent value for each individual patient\n",
    "imp = Imputer(missing_values = 'NaN', strategy = 'most_frequent', axis = 0)\n",
    "global_means = labels_train.iloc[:,2:].mean() # will be used when the whole column is missing\n",
    "for key, value in dic_train.items():\n",
    "        #Get the most frequent value in each column of each patient\n",
    "        subLbl = labels_train.iloc[value,:]\n",
    "        modes = subLbl.mode().iloc[0,:]\n",
    "        diag = subLbl.iloc[:,2]*4+subLbl.iloc[:,3]*2+subLbl.iloc[:,4]\n",
    "        diag_mode = diag.mode()\n",
    "        #If the most frequent element of a column is a nan, that means all elemetns are nan in that column, so we will have to impute missing data in that column from input X.\n",
    "        if diag_mode.size == 0: # If there is no diagnostic result, impute with the most typical situation\n",
    "            modes['CN_Diag'] = 0\n",
    "            modes['MCI_Diag'] = 1\n",
    "            modes['AD_Diag'] = 0 \n",
    "        else:\n",
    "            b = bin(int(diag_mode.iloc[0]))[2:].zfill(3)\n",
    "            modes['CN_Diag'] = b[0]\n",
    "            modes['MCI_Diag'] = b[1]\n",
    "            modes['AD_Diag'] = b[2] \n",
    "    \n",
    "        if np.isnan(modes['ADAS13']):\n",
    "            modes['ADAS13'] = global_means['ADAS13']\n",
    "        if np.isnan(modes['Ventricles_Norm']):\n",
    "            modes['Ventricles_Norm'] = global_means['Ventricles_Norm']\n",
    "        if np.isnan(modes['MMSE']):\n",
    "            modes['MMSE'] = global_means['MMSE']\n",
    "\n",
    "        # impute missing data with either the most frequent item in this column for this patient or population mode if no record is found for this patient.    \n",
    "        y_train_individual = labels_train.iloc[value,:].append(modes)\n",
    "        imp = imp.fit(y_train_individual.values[:,2:]) \n",
    "        temp = imp.transform(y_train_individual.values[:,2:])\n",
    "        labels_train.iloc[value,2:] = temp[:-1,:]\n",
    "\n",
    "colNames = labels_train.columns.tolist()\n",
    "newColNames = [colNames[1],colNames[0]]+ colNames[2:]\n",
    "labels_train = labels_train[newColNames]\n",
    "y_train = labels_train.values\n",
    "yAttributes = labels_train.columns.values\n",
    "\n",
    "# Work on labels_test: convert dates to number of days relative to the initial date, and then bind to the rest of data and do imputation\n",
    "idxPTIDTable_test = pd.concat([pd.DataFrame(labels_test.index.values),labels_test['PTID_Key']],axis=1)\n",
    "dic_test = idxPTIDTable_test.groupby('PTID_Key').groups\n",
    "dateY_test_raw = pd.to_datetime(labels_test['Date'],format=\"%Y-%m-%d\",errors='coerce')\n",
    "dateY_test = np.zeros(dateY_test_raw.size)\n",
    "dateY_test[:] = np.nan\n",
    "for key, value in dic_test.items():\n",
    "    allDates = dateY_test_raw[value] # Get all dates in column i about patient with ID 'key'\n",
    "    firstDate = allDates.min()\n",
    "    validDateIdx = np.where(~pd.isnull(allDates))[0]\n",
    "    for j in range(len(validDateIdx)):\n",
    "        dateY_test[value[j]] = (allDates[value[j]]-firstDate).days\n",
    "labels_test['Date'] = dateY_test\n",
    "colNames = labels_test.columns.tolist()\n",
    "newColNames = [colNames[1],colNames[0]]+ colNames[2:] # switch PTID_Key to the first column\n",
    "labels_test = labels_test[newColNames]\n",
    "\n",
    "# Build a row index lookup table for each patient from Data to labels_train/labels_test\n",
    "ptid_roster = Data['PTID_Key']\n",
    "ptid_roster = ptid_roster.dropna(how='all') \n",
    "ptid_roster = ptid_roster.unique()\n",
    "ptid_roster.sort()\n",
    "ptid_roster = ptid_roster.astype(int)\n",
    "\n",
    "#train data\n",
    "ptid_train = labels_train['PTID_Key'].unique()\n",
    "ptid_train.sort()\n",
    "#test data\n",
    "ptid_test = labels_test['PTID_Key'].unique()\n",
    "ptid_test.sort()\n",
    "#validation data\n",
    "ptid_validation = target['PTID_Key'].unique()\n",
    "ptid_validation.sort()\n",
    "\n",
    "\n",
    "# Separate input data for traning and test sets \n",
    "X_train_rowIdx = []\n",
    "for i in ptid_train:\n",
    "    X_train_rowIdx.extend(dic[i].tolist())\n",
    "    \n",
    "X_test_rowIdx = []\n",
    "for i in ptid_test:\n",
    "    X_test_rowIdx.extend(dic[i].tolist())\n",
    "\n",
    "X_train = Data.iloc[X_train_rowIdx,:]\n",
    "X_train = X_train.reset_index(drop=True)\n",
    "X_test = Data.iloc[X_test_rowIdx,:]\n",
    "X_test = X_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get last patient visit from input data\n",
    "patient_visit = {}\n",
    "#itereate through dataset and get all patients visits\n",
    "for index, row in dataset.iterrows():\n",
    "    if row[0] not in patient_visit.keys():\n",
    "        patient_visit[row[0]] = list()\n",
    "    patient_visit[row[0]].append(row[1])\n",
    "#get the last patient visit from the patient visit dictionary\n",
    "first_patient_visit = {}    \n",
    "for key, val in patient_visit.items():\n",
    "    first_patient_visit[key] = max(patient_visit[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "og_train = pd.read_csv('TADPOLE_TargetData_train.csv')\n",
    "og_test = pd.read_csv('TADPOLE_TargetData_test.csv')\n",
    "target = pd.read_csv('TADPOLE_PredictTargetData_valid.csv')\n",
    "\n",
    "#Sort all datasets by ascending patient ID.\n",
    "og_train = og_train.sort_values('PTID_Key')\n",
    "og_test = og_test.sort_values('PTID_Key')\n",
    "\n",
    "# Reindex rows\n",
    "og_train = og_train.reset_index(drop=True)\n",
    "og_test = og_test.reset_index(drop=True)\n",
    "\n",
    "#convert Date to pd datetime format\n",
    "og_train['Date'] = pd.to_datetime(og_train['Date'],format=\"%m/%d/%y\",errors='coerce')\n",
    "og_test['Date'] = pd.to_datetime(og_test['Date'],format=\"%Y-%m-%d\",errors='coerce')\n",
    "target['Date'] = pd.to_datetime(target['Date'],format=\"%m/%d/%y\",errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert dates in train, test and target to absolute years passed from first visit\n",
    "abs_days_train = np.zeros((og_train.shape[0],))\n",
    "for index, row in og_train.iterrows():\n",
    "    current_ptid = row[1]\n",
    "    abs_day_conv = row[0] - first_patient_visit[current_ptid]#convert to days \n",
    "    abs_days_train[index] = float(abs_day_conv.days/360) - 0.3 #makes sure that it's >0.8 of a year in order for it to be a year\n",
    "    abs_days_train[index] = abs(round(abs_days_train[index])) #round the value so they're integral steps\n",
    "og_train['Date'] = abs_days_train #set Date column to year converted column\n",
    "\n",
    "#same steps as above repeated for test and target dataframes\n",
    "abs_days_test = np.zeros((og_test.shape[0],))\n",
    "for index, row in og_test.iterrows():\n",
    "    current_ptid = row[1]\n",
    "    abs_day_conv = row[0] - first_patient_visit[current_ptid]\n",
    "    abs_days_test[index] = float(abs_day_conv.days/360) - 0.3\n",
    "    abs_days_test[index] = abs(round(abs_days_test[index]))\n",
    "og_test['Date'] = abs_days_test\n",
    "\n",
    "abs_days_target = np.zeros((target.shape[0],))\n",
    "for index, row in target.iterrows():\n",
    "    current_ptid = row[1]\n",
    "    abs_day_conv = row[0] - first_patient_visit[current_ptid]\n",
    "    abs_days_target[index] = float(abs_day_conv.days/360) - 0.3\n",
    "    abs_days_target[index] = abs(round(abs_days_target[index]))\n",
    "target['Date'] = abs_days_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set new relative dates for main train and test data frame\n",
    "labels_train['Date'] = og_train['Date']\n",
    "labels_test['Date'] = og_test['Date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sort years and space them\n",
    "\n",
    "#create dictionary where key -> y_feats\n",
    "y_train_dict_years = {}\n",
    "for index, row in labels_train.iterrows():\n",
    "    current_ptid = row[0]\n",
    "    if current_ptid not in y_train_dict_years.keys():\n",
    "        y_train_dict_years[current_ptid] = list()\n",
    "    y_train_dict_years[current_ptid].append(row)\n",
    "\n",
    "#create dictionary where ptid maps to sorted time steps\n",
    "time_sorted_y_train = {}\n",
    "for key, val in y_train_dict_years.items():\n",
    "    time_sorted_y_train[key] = list()\n",
    "    y_time_list = []\n",
    "    vals_to_add = len(val)\n",
    "    for i in range(len(val)):\n",
    "        y_time_list.append(val[i][1])\n",
    "    y_time_list.sort()\n",
    "    #keep spining in while loop til values are sorted\n",
    "    index = 0\n",
    "    while(index != vals_to_add):\n",
    "        for i in range(len(val)):\n",
    "            if val[i][1] == y_time_list[index]:\n",
    "                time_sorted_y_train[key].append(val[i])\n",
    "                break\n",
    "        index+=1   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create new column titles for time-series modeled train data\n",
    "y_train_num_samples = len(ptid_train)\n",
    "y_train_cats = list(labels_train.columns)\n",
    "y_train_cats.remove('Date')\n",
    "y_train_time_shifted_cats = list()\n",
    "for cats in y_train_cats:\n",
    "    if cats == 'PTID_Key':\n",
    "        y_train_time_shifted_cats.append(cats)\n",
    "    else:\n",
    "        y_train_time_shifted_cats.append(str(cats) + '_t')\n",
    "        y_train_time_shifted_cats.append(str(cats) + '_t+1')\n",
    "        y_train_time_shifted_cats.append(str(cats) + '_t+2')\n",
    "        y_train_time_shifted_cats.append(str(cats) + '_t+3')\n",
    "        y_train_time_shifted_cats.append(str(cats) + '_t+4')\n",
    "        y_train_time_shifted_cats.append(str(cats) + '_t+5')\n",
    "y_train_num_feats = len(y_train_time_shifted_cats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make dictionary to map ptid -> all entries in train data\n",
    "y_train_ptid = {}\n",
    "for index, row in labels_train.iterrows():\n",
    "    if row[0] not in y_train_ptid.keys():\n",
    "        y_train_ptid[row[0]] = list()\n",
    "    y_train_ptid[row[0]].append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make dictionary that maps ptid -> entries indexed in a list by timesteps\n",
    "y_train_timestep = {}\n",
    "for key, val in y_train_ptid.items():\n",
    "    y_train_timestep[key] = {}\n",
    "    y_train_timestep[key]['timestep'] = list()\n",
    "    #indexes in 'TS' mapped to timestep where 0=t, 1=t+1, 2=t+2, 3=t+3, 4=t+4, 5=t+5\n",
    "    y_train_timestep[key]['TS'] = [[],[],[],[],[],[]]\n",
    "    for entry in val:\n",
    "        #check if timestep is in the dictionary\n",
    "        if int(entry[1]) not in y_train_timestep[key]['timestep']:\n",
    "            y_train_timestep[key]['timestep'].append(entry[1]) #add entry to list of timesteps\n",
    "        y_train_timestep[key]['TS'][int(entry[1])].append(entry) #add y entry to corresponding timestep index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sort entries by timesteps\n",
    "y_timestep_dict = {}\n",
    "y_timestep_dict[0] = list()\n",
    "y_timestep_dict[1] = list()\n",
    "y_timestep_dict[2] = list()\n",
    "y_timestep_dict[3] = list()\n",
    "y_timestep_dict[4] = list()\n",
    "y_timestep_dict[5] = list()\n",
    "#add entries to each dictionary depending on their timesteps\n",
    "for index, row in labels_train.iterrows():\n",
    "    key = int(row[1])\n",
    "    y_timestep_dict[key].append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPUTE ALL OF Y TRAIN'S MISSING \n",
    "condensed_imputed_y_train = {}\n",
    "for key, val in y_train_timestep.items():\n",
    "    condensed_imputed_y_train[key] = np.zeros((6,8))\n",
    "    last_ts = val['timestep']\n",
    "    all_ts = val['TS']\n",
    "    #iterate through list where index corresponds to timestep\n",
    "    for i in range(len(all_ts)):\n",
    "        total_entry = len(all_ts[i])\n",
    "        \n",
    "        #if there is no entry for the year, use the diagnosis for the last patient visit\n",
    "        if total_entry == 0:\n",
    "            condensed_entry = np.zeros((8,))\n",
    "            \n",
    "            #use population mean at the timestep to get global averages to impute missing timesteps for patients\n",
    "            timestep_entries = y_timestep_dict[i]\n",
    "            running_adas = 0\n",
    "            running_vn = 0\n",
    "            running_mmse = 0\n",
    "            #get population mean over that timestep's entries\n",
    "            for entry in timestep_entries:\n",
    "                running_adas += entry[5]\n",
    "                running_vn += entry[6]\n",
    "                running_mmse += entry[7]\n",
    "            running_adas /= len(timestep_entries)\n",
    "            running_vn /= len(timestep_entries)\n",
    "            running_mmse /= len(timestep_entries)\n",
    "            #set entries to their averages\n",
    "            condensed_entry[5] = running_adas\n",
    "            condensed_entry[6] = running_vn\n",
    "            condensed_entry[7] = running_mmse\n",
    "            \n",
    "            #get last year logged\n",
    "            patient_last_year = max(y_train_timestep[key]['timestep'])\n",
    "            #if there are missing intermediate values\n",
    "            if i < patient_last_year:\n",
    "                ptid_last_visit_entry = all_ts[int(patient_last_year)]\n",
    "                condensed_entry[2:5] = ptid_last_visit_entry[0][2:5]\n",
    "                condensed_entry[0] = key\n",
    "                condensed_entry[1] = i \n",
    "            #timestep happens after the last visit\n",
    "            else:\n",
    "                condensed_entry[0] = key\n",
    "                condensed_entry[1] = i\n",
    "                ptid_last_visit_entry = all_ts[int(patient_last_year)]\n",
    "                condensed_entry[2:5] = ptid_last_visit_entry[0][2:5]\n",
    "        #if theres more than one entry for the year, average over all the entries in the timestep\n",
    "        elif total_entry > 0:\n",
    "            condensed_entry = np.zeros((8,))\n",
    "            condensed_entry[0] = key\n",
    "            condensed_entry[1] = i\n",
    "            condensed_entry[2:5] = all_ts[i][0][2:5]\n",
    "            running_adas = 0\n",
    "            running_vn = 0\n",
    "            running_mmse = 0\n",
    "\n",
    "            for entry in all_ts[i]:\n",
    "                running_adas += entry[5]\n",
    "                running_vn += entry[6]\n",
    "                running_mmse += entry[7]\n",
    "\n",
    "            running_adas /= total_entry\n",
    "            running_vn /= total_entry\n",
    "            running_mmse /= total_entry\n",
    "            condensed_entry[5] = running_adas\n",
    "            condensed_entry[6] = running_vn\n",
    "            condensed_entry[7] = running_mmse\n",
    "        #if theres no entry for the year, use population mean for target variables & last diagnosis patient\n",
    "        # for values in between\n",
    "        elif total_entry == 1:\n",
    "            condensed_entry = np.zeros((8,))\n",
    "            condensed_entry = all_ts[i][0]\n",
    "                    \n",
    "        condensed_imputed_y_train[key][i] = condensed_entry\n",
    "                    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOAD NUMPY MATRIX WITH Y_TRAIN TIME-SERIES FEATURES\n",
    "y_train_time_series = np.zeros((y_train_num_samples, y_train_num_feats))\n",
    "y_train_ts_ind = 0\n",
    "for key, val in condensed_imputed_y_train.items():\n",
    "    set_key = 0\n",
    "    #iterate through entries per key\n",
    "    y_train_entry_array = np.zeros((y_train_num_feats,))\n",
    "    for entry in val:\n",
    "        current_timestep = entry[1]\n",
    "        entry_cn = entry[2]\n",
    "        entry_mci = entry[3]\n",
    "        entry_ad = entry[4]\n",
    "        entry_adas = entry[5]\n",
    "        entry_vn = entry[6]\n",
    "        entry_mmse = entry[7]\n",
    "        #set patient id\n",
    "        if set_key == 0:\n",
    "            y_train_entry_array[0] = entry[0]\n",
    "            set_key = 1\n",
    "        if current_timestep==0:\n",
    "            ts_offset = 1\n",
    "            y_train_entry_array[1] = entry_cn\n",
    "            y_train_entry_array[7] = entry_mci\n",
    "            y_train_entry_array[13] = entry_ad\n",
    "            y_train_entry_array[19] = entry_adas\n",
    "            y_train_entry_array[25] = entry_vn\n",
    "            y_train_entry_array[31] = entry_mmse\n",
    "        elif current_timestep==1:\n",
    "            y_train_entry_array[2] = entry_cn\n",
    "            y_train_entry_array[8] = entry_mci\n",
    "            y_train_entry_array[14] = entry_ad\n",
    "            y_train_entry_array[20] = entry_adas\n",
    "            y_train_entry_array[26] = entry_vn\n",
    "            y_train_entry_array[32] = entry_mmse            \n",
    "\n",
    "        elif current_timestep==2:\n",
    "            y_train_entry_array[3] = entry_cn\n",
    "            y_train_entry_array[9] = entry_mci\n",
    "            y_train_entry_array[15] = entry_ad\n",
    "            y_train_entry_array[21] = entry_adas\n",
    "            y_train_entry_array[27] = entry_vn\n",
    "            y_train_entry_array[33] = entry_mmse\n",
    "        elif current_timestep==3:\n",
    "            y_train_entry_array[4] = entry_cn\n",
    "            y_train_entry_array[10] = entry_mci\n",
    "            y_train_entry_array[16] = entry_ad\n",
    "            y_train_entry_array[22] = entry_adas\n",
    "            y_train_entry_array[28] = entry_vn\n",
    "            y_train_entry_array[34] = entry_mmse\n",
    "        elif current_timestep==4:\n",
    "            y_train_entry_array[5] = entry_cn\n",
    "            y_train_entry_array[11] = entry_mci\n",
    "            y_train_entry_array[17] = entry_ad\n",
    "            y_train_entry_array[23] = entry_adas\n",
    "            y_train_entry_array[29] = entry_vn\n",
    "            y_train_entry_array[35] = entry_mmse\n",
    "        elif current_timestep==5:\n",
    "            y_train_entry_array[6] = entry_cn\n",
    "            y_train_entry_array[12] = entry_mci\n",
    "            y_train_entry_array[18] = entry_ad\n",
    "            y_train_entry_array[24] = entry_adas\n",
    "            y_train_entry_array[30] = entry_vn\n",
    "            y_train_entry_array[36] = entry_mmse \n",
    "    y_train_time_series[y_train_ts_ind] = y_train_entry_array\n",
    "    y_train_ts_ind +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CREATE Y_TRAIN DATAFRAME FOR ACCESSING DATA FROM IT MORE CONVENIENT\n",
    "y_train_ts_df = pd.DataFrame(data=y_train_time_series, columns=y_train_time_shifted_cats)\n",
    "\n",
    "#Sort all datasets by ascending patient ID.\n",
    "y_train_ts_df = y_train_ts_df.sort_values('PTID_Key')\n",
    "\n",
    "# Reindex rows\n",
    "y_train_ts_df = y_train_ts_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CREATE TIME SERIES INPUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#INSERT ELAPSED DAYS INTO DATASET TO START CONVERTING PATIENT VISITS INTO TIME-SERIES MODEL\n",
    "time_series_data = Data.insert(1, 'Elapsed Days', dateX[:,0] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create PTID Dictionary, maps PTID -> all visits\n",
    "PTID_Dict = {}\n",
    "for i in range(len(Data)):\n",
    "    if Data.values[i][0] not in PTID_Dict.keys():\n",
    "        PTID_Dict[Data.values[i][0]] = list()\n",
    "        PTID_Dict[Data.values[i][0]].append(Data.values[i])\n",
    "    else:\n",
    "        PTID_Dict[Data.values[i][0]].append(Data.values[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#sort PTID entries by time of visit, val is a list of patient visits\n",
    "time_sorted_PTID_dict = {}\n",
    "for key, val in PTID_Dict.items():\n",
    "    time_sorted_PTID_dict[key] = list()\n",
    "    time_list = []\n",
    "    vals_to_add = len(val)\n",
    "    for i in range(len(val)):\n",
    "        time_list.append(val[i][1])\n",
    "    time_list.sort()\n",
    "    #while loop to sort bruteforce\n",
    "    index = 0\n",
    "    while(index != vals_to_add):\n",
    "        for i in range(len(val)):\n",
    "            if val[i][1] == time_list[index]:\n",
    "                time_sorted_PTID_dict[key].append(val[i])\n",
    "                break\n",
    "        index+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert time elapsed from days -> years\n",
    "year_ptid_dict={}\n",
    "for key,val in time_sorted_PTID_dict.items():\n",
    "    year_ptid_dict[key] = list()\n",
    "    for i in range(len(val)):  \n",
    "        year_ptid_dict[key].append(val[i])\n",
    "        year_conv = val[i][1]/360\n",
    "        year_ptid_dict[key][i][1] = year_conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get visits that are spaced a year apart\n",
    "import math\n",
    "condensed_time_series_dict = {}\n",
    "for key, val in year_ptid_dict.items():\n",
    "    condensed_time_series_dict[key] = list()\n",
    "    last_added = 0\n",
    "    condensed_time_series_dict[key].append(val[0])\n",
    "    if len(val)>1:\n",
    "        for i in range(len(val)-1):\n",
    "            tmp = val[i+1][1]-.3 #var to check if year is at least .85\n",
    "            if (round(tmp)) >= (last_added + 1):\n",
    "                condensed_time_series_dict[key].append(val[i+1])\n",
    "                last_added = round(tmp)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get last two visits of each patient \n",
    "last_two_visits_time_series_dict = {}\n",
    "for key, val in condensed_time_series_dict.items():\n",
    "    last_two_visits_time_series_dict[key]=list()\n",
    "    for entry in val[-2:]: \n",
    "        last_two_visits_time_series_dict[key].append(np.delete(entry, 1))\n",
    "    if len(last_two_visits_time_series_dict[key])==1:\n",
    "        last_two_visits_time_series_dict[key].append(np.delete(val,1))\n",
    "        \n",
    "#pack dictionary into dataframe for inspection\n",
    "last_two_visits_df = pd.DataFrame(data=last_two_visits_time_series_dict)\n",
    "last_two_visits_df = last_two_visits_df.transpose()\n",
    "last_two_visits_df.columns = ['t-1', 't']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#last_two_visits_df[i][j], where i = ptid-1, j=time step where j=0=(t-1), j=1=t\n",
    "num_ptids = last_two_visits_df.shape[0]\n",
    "num_feats = Data.shape[1]\n",
    "\n",
    "#MAKE MATRICES TO HOLD FEATURES\n",
    "t_1_ptid_feats = np.zeros((num_ptids,num_feats-1))\n",
    "t_ptid_feats = np.zeros((num_ptids,num_feats-1))\n",
    "\n",
    "\n",
    "for key, val in last_two_visits_time_series_dict.items():\n",
    "    ind = int(key-1)\n",
    "    t_1_ptid_feats[ind]=val[0]\n",
    "    t_ptid_feats[ind] = val[1]\n",
    "\n",
    "#REMOVE DAYS ELAPSED B/C IRRELEVANT FEATURE NOW\n",
    "cats = list(Data.columns)\n",
    "cats.remove('Elapsed Days')\n",
    "\n",
    "#CREATE NEW COLUMN NAMES FOR NEW TIME-SERIES FEATURES\n",
    "time_shifted_cats = []\n",
    "for i in range(len(cats)):\n",
    "    if cats[i] == 'PTID_Key':\n",
    "        time_shifted_cats.append(cats[i])\n",
    "    else:\n",
    "        time_shifted_cats.append(str(cats[i])+'_t-1')\n",
    "        time_shifted_cats.append(str(cats[i])+'_t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODIFY INPUT VECTOR ACCORDING TO TRAIN, TEST AND VALIDATION "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CREATE TIME-SERIES MODELED HISTORICAL PATIENT DATA\n",
    "time_shifted_feat_matrix = np.zeros((num_ptids, len(time_shifted_cats)))\n",
    "for i in range(len(dataset.columns)):\n",
    "    if i==0:\n",
    "        time_shifted_feat_matrix[:,i] = t_ptid_feats[:,i]\n",
    "    else:\n",
    "        #t-1\n",
    "        time_shifted_feat_matrix[:,2*i-1] = t_1_ptid_feats[:,i]\n",
    "        #t\n",
    "        time_shifted_feat_matrix[:,2*i] = t_ptid_feats[:,i]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#modify input for training\n",
    "Xts_train_data = np.zeros((len(ptid_train), len(time_shifted_cats)))\n",
    "for i in range(len(ptid_train)):\n",
    "    ptid_index = ptid_train[i]-1\n",
    "    Xts_train_data[i] = time_shifted_feat_matrix[ptid_index]\n",
    "Xts_train_df = pd.DataFrame(data=Xts_train_data, columns=time_shifted_cats)\n",
    "\n",
    "#modify input for testing\n",
    "Xts_test_data = np.zeros((len(ptid_test),len(time_shifted_cats)))\n",
    "for i in range(len(ptid_test)):\n",
    "    ptid_index = ptid_test[i]-1\n",
    "    Xts_test_data[i] = time_shifted_feat_matrix[ptid_index]\n",
    "Xts_test_df = pd.DataFrame(data=Xts_test_data, columns=time_shifted_cats)\n",
    "\n",
    "#modify input for validation\n",
    "Xts_target_data = np.zeros((len(ptid_validation), len(time_shifted_cats)))\n",
    "for i in range(len(ptid_validation)):\n",
    "    ptid_index = ptid_validation[i]-1\n",
    "    Xts_target_data[i] = time_shifted_feat_matrix[ptid_index]\n",
    "Xts_target_df = pd.DataFrame(data=Xts_target_data, columns=time_shifted_cats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc = StandardScaler()\n",
    "X_train_scaled = sc.fit_transform(Xts_train_df.values[:,1:])\n",
    "X_test_scaled = sc.fit_transform(Xts_test_df.values[:,1:])\n",
    "X_validation_scaled = sc.fit_transform(Xts_target_df.values[:,1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Support Vector Classifier/Regression\n",
    "\n",
    "Each estimator has 6 different trained models, one model for each timestep starting from the current year to five years out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create ptid -> years for test and validation set\n",
    "test_ptid_prediction = {}\n",
    "target_ptid_prediction = {}\n",
    "#itereate through test \n",
    "for index, row in labels_test.iterrows():\n",
    "    if row[0] not in test_ptid_prediction.keys():\n",
    "        test_ptid_prediction[row[0]] = list()\n",
    "    test_ptid_prediction[row[0]].append(row[1]) #add year to prediction\n",
    "\n",
    "for index, row in target.iterrows():\n",
    "    if row[1] not in target_ptid_prediction.keys():\n",
    "        target_ptid_prediction[row[1]] = list()\n",
    "    target_ptid_prediction[row[1]].append(row[0]) #add year to prediction\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### MMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVR(C=100.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma='auto',\n",
       "  kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "mmse_models = [svm.SVR(C=1e2),svm.SVR(C=1e2),svm.SVR(C=1e2),svm.SVR(C=1e2),svm.SVR(C=1e2),svm.SVR(C=1e2)]\n",
    "y_mmse_t = y_train_ts_df.values[:,31]\n",
    "y_mmse_tp1 = y_train_ts_df.values[:,32]\n",
    "y_mmse_tp2 = y_train_ts_df.values[:,33]\n",
    "y_mmse_tp3 = y_train_ts_df.values[:,34]\n",
    "y_mmse_tp4 = y_train_ts_df.values[:,35]\n",
    "y_mmse_tp5 = y_train_ts_df.values[:,36]\n",
    "\n",
    "mmse_models[0].fit(X_train_scaled, y_mmse_t)\n",
    "mmse_models[1].fit(X_train_scaled, y_mmse_tp1)\n",
    "mmse_models[2].fit(X_train_scaled, y_mmse_tp2)\n",
    "mmse_models[3].fit(X_train_scaled, y_mmse_tp3)\n",
    "mmse_models[4].fit(X_train_scaled, y_mmse_tp4)\n",
    "mmse_models[5].fit(X_train_scaled, y_mmse_tp5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Ventricle Norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma='auto',\n",
       "  kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "vn_models = [svm.SVR(),svm.SVR(),svm.SVR(),svm.SVR(),svm.SVR(),svm.SVR()]\n",
    "y_vn_t = y_train_ts_df.values[:,25]\n",
    "y_vn_tp1 = y_train_ts_df.values[:,26]\n",
    "y_vn_tp2 = y_train_ts_df.values[:,27]\n",
    "y_vn_tp3 = y_train_ts_df.values[:,28]\n",
    "y_vn_tp4 = y_train_ts_df.values[:,29]\n",
    "y_vn_tp5 = y_train_ts_df.values[:,30]\n",
    "\n",
    "vn_models[0].fit(X_train_scaled, y_vn_t)\n",
    "vn_models[1].fit(X_train_scaled, y_vn_tp1)\n",
    "vn_models[2].fit(X_train_scaled, y_vn_tp2)\n",
    "vn_models[3].fit(X_train_scaled, y_vn_tp3)\n",
    "vn_models[4].fit(X_train_scaled, y_vn_tp4)\n",
    "vn_models[5].fit(X_train_scaled, y_vn_tp5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Diagnosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diagnosis_to_classes(y_diagnosis_cols, num_entries):\n",
    "    condensed_classes = np.zeros((num_entries,))\n",
    "    if num_entries==1:\n",
    "        check_equivalence_healthy = y_diagnosis_cols == np.array([1,0,0])\n",
    "        check_equivalence_mci = y_diagnosis_cols == np.array([0,1,0])\n",
    "        check_equivalence_ad = y_diagnosis_cols == np.array([0,0,1])\n",
    "        if check_equivalence_healthy.all() == True:\n",
    "            condensed_classes = 0\n",
    "        elif check_equivalence_mci.all() == True:\n",
    "            condensed_classes = 1\n",
    "        elif check_equivalence_ad.all() == True:\n",
    "            condensed_classes = 2    \n",
    "    else:\n",
    "        for i in range(len(condensed_classes)):\n",
    "            check_equivalence_healthy = y_diagnosis_cols[i] == np.array([1,0,0])\n",
    "            check_equivalence_mci = y_diagnosis_cols[i] == np.array([0,1,0])\n",
    "            check_equivalence_ad = y_diagnosis_cols[i] == np.array([0,0,1])\n",
    "            if check_equivalence_healthy.all() == True:\n",
    "                condensed_classes[i] = 0\n",
    "            elif check_equivalence_mci.all() == True:\n",
    "                condensed_classes[i] = 1\n",
    "            elif check_equivalence_ad.all() == True:\n",
    "                condensed_classes[i] = 2        \n",
    "    return condensed_classes\n",
    "\n",
    "\n",
    "def classes_to_diagnosis(y_diagnosis_col):\n",
    "    condensed_classes = np.zeros((1, 3))\n",
    "    if y_diagnosis_col == 0:\n",
    "        condensed_classes = np.array([1,0,0])\n",
    "    elif y_diagnosis_col == 1:\n",
    "        condensed_classes = np.array([0,1,0])\n",
    "    elif y_diagnosis_col == 2:\n",
    "        condensed_classes = np.array([0,0,1])     \n",
    "    return condensed_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=2000, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diagnosis_models = [svm.SVC(max_iter=2000), svm.SVC(max_iter=2000), svm.SVC(max_iter=2000), svm.SVC(max_iter=2000), svm.SVC(max_iter=2000), svm.SVC(max_iter=2000)]\n",
    "y_diag_t = np.column_stack( (y_train_ts_df.values[:,1], y_train_ts_df.values[:,7], y_train_ts_df.values[:,13]) )\n",
    "y_diag_tp1 = np.column_stack( (y_train_ts_df.values[:,2], y_train_ts_df.values[:,8], y_train_ts_df.values[:,14]) )\n",
    "y_diag_tp2 = np.column_stack( (y_train_ts_df.values[:,3], y_train_ts_df.values[:,9], y_train_ts_df.values[:,15]) )\n",
    "y_diag_tp3 = np.column_stack( (y_train_ts_df.values[:,4], y_train_ts_df.values[:,10], y_train_ts_df.values[:,16]) )\n",
    "y_diag_tp4 = np.column_stack( (y_train_ts_df.values[:,5], y_train_ts_df.values[:,11], y_train_ts_df.values[:,17]) )\n",
    "y_diag_tp5 = np.column_stack( (y_train_ts_df.values[:,6], y_train_ts_df.values[:,12], y_train_ts_df.values[:,18]) )\n",
    "\n",
    "y_diag_t_classes = diagnosis_to_classes(y_diag_t, y_diag_t.shape[0])\n",
    "y_diag_tp1_classes = diagnosis_to_classes(y_diag_tp1, y_diag_tp1.shape[0])\n",
    "y_diag_tp2_classes = diagnosis_to_classes(y_diag_tp2, y_diag_tp2.shape[0])\n",
    "y_diag_tp3_classes = diagnosis_to_classes(y_diag_tp3, y_diag_tp3.shape[0])\n",
    "y_diag_tp4_classes = diagnosis_to_classes(y_diag_tp4, y_diag_tp4.shape[0])\n",
    "y_diag_tp5_classes = diagnosis_to_classes(y_diag_tp5, y_diag_tp5.shape[0])\n",
    "\n",
    "diagnosis_models[0].fit(X_train_scaled, y_diag_t_classes)\n",
    "diagnosis_models[1].fit(X_train_scaled, y_diag_tp1_classes)\n",
    "diagnosis_models[2].fit(X_train_scaled, y_diag_tp2_classes)\n",
    "diagnosis_models[3].fit(X_train_scaled, y_diag_tp3_classes)\n",
    "diagnosis_models[4].fit(X_train_scaled, y_diag_tp4_classes)\n",
    "diagnosis_models[5].fit(X_train_scaled, y_diag_tp5_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ADAS13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVR(C=100000000.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1,\n",
       "  gamma='auto', kernel='rbf', max_iter=-1, shrinking=True, tol=0.001,\n",
       "  verbose=False)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adas13_models = [svm.SVR(C=1e8),svm.SVR(C=1e8),svm.SVR(C=1e8),svm.SVR(C=1e8),svm.SVR(C=1e8),svm.SVR(C=1e8)]\n",
    "y_adas13_t = y_train_ts_df.values[:,19]\n",
    "y_adas13_tp1 = y_train_ts_df.values[:,20]\n",
    "y_adas13_tp2 = y_train_ts_df.values[:,21]\n",
    "y_adas13_tp3 = y_train_ts_df.values[:,22]\n",
    "y_adas13_tp4 = y_train_ts_df.values[:,23]\n",
    "y_adas13_tp5 = y_train_ts_df.values[:,24]\n",
    "\n",
    "adas13_models[0].fit(X_train_scaled, y_adas13_t)\n",
    "adas13_models[1].fit(X_train_scaled, y_adas13_tp1)\n",
    "adas13_models[2].fit(X_train_scaled, y_adas13_tp2)\n",
    "adas13_models[3].fit(X_train_scaled, y_adas13_tp3)\n",
    "adas13_models[4].fit(X_train_scaled, y_adas13_tp4)\n",
    "adas13_models[5].fit(X_train_scaled, y_adas13_tp5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making Prediction CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make prediction matrices\n",
    "predicted_diagnosis_test = []\n",
    "predicted_adas13_test = []\n",
    "predicted_mmse_test = []\n",
    "predicted_vn_test = []\n",
    "\n",
    "predicted_diagnosis_valid = []\n",
    "predicted_adas13_valid = []\n",
    "predicted_mmse_valid = []\n",
    "predicted_vn_valid = []\n",
    "for i in range(6):\n",
    "    predicted_diagnosis_test.append(diagnosis_models[i].predict(X_test_scaled))\n",
    "    predicted_adas13_test.append(adas13_models[i].predict(X_test_scaled))\n",
    "    predicted_mmse_test.append(mmse_models[i].predict(X_test_scaled))\n",
    "    predicted_vn_test.append(vn_models[i].predict(X_test_scaled))\n",
    "    \n",
    "    predicted_diagnosis_valid.append(diagnosis_models[i].predict(X_validation_scaled))\n",
    "    predicted_adas13_valid.append(adas13_models[i].predict(X_validation_scaled))\n",
    "    predicted_mmse_valid.append(mmse_models[i].predict(X_validation_scaled))\n",
    "    predicted_vn_valid.append(vn_models[i].predict(X_validation_scaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PTID indexes in the prediction matrices \n",
    "X_test_indices = list(Xts_test_df['PTID_Key'])\n",
    "X_valid_indices = list(Xts_target_df['PTID_Key']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get original dates of patient visits to create csv\n",
    "original_test_dates_df = pd.read_csv('TADPOLE_TargetData_test.csv')\n",
    "original_validation_dates_df = pd.read_csv('TADPOLE_PredictTargetData_valid.csv')\n",
    "\n",
    "#keys to iterate over in the dictionaries in order\n",
    "original_test_order = []\n",
    "original_validation_order = []\n",
    "for index, row in original_test_dates_df.iterrows():\n",
    "    if row[1] not in original_test_order:\n",
    "        original_test_order.append(row[1])\n",
    "        \n",
    "for index, row in original_validation_dates_df.iterrows():\n",
    "    if row[1] not in original_validation_order:\n",
    "        original_validation_order.append(row[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#indices to set results matrix\n",
    "ind_test = 0 \n",
    "ind_valid = 0\n",
    "\n",
    "#np matrices to hold results\n",
    "final_prediction_test = np.zeros( (original_test_dates_df.shape[0],7) )\n",
    "final_prediction_validation = np.zeros( (original_validation_dates_df.shape[0], 7) ) \n",
    "\n",
    "#iterate through keys in order they appear in original csv\n",
    "for key in original_test_order:\n",
    "    #get list of all timesteps to predict\n",
    "    timesteps_to_predict = test_ptid_prediction[key]\n",
    "    #the the corresponding index for the prediction matrix\n",
    "    corr_ptid_index = X_test_indices.index(key)\n",
    "    for timestep in timesteps_to_predict:\n",
    "    \n",
    "        prediction_entry = np.zeros((7,))\n",
    "        prediction_entry[0] = key\n",
    "        prediction_entry[1:4] = classes_to_diagnosis(predicted_diagnosis_test[int(timestep)][corr_ptid_index])\n",
    "        prediction_entry[4] = predicted_adas13_test[int(timestep)][corr_ptid_index]\n",
    "        prediction_entry[5] = predicted_vn_test[int(timestep)][corr_ptid_index]\n",
    "        prediction_entry[6] = predicted_mmse_test[int(timestep)][corr_ptid_index]\n",
    "        \n",
    "        final_prediction_test[ind_test] = prediction_entry\n",
    "        ind_test += 1\n",
    "        \n",
    "\n",
    "#iterate through keys in order they appear in original csv\n",
    "for key in original_validation_order:\n",
    "    #get list of all timesteps to predict\n",
    "    timesteps_to_predict = target_ptid_prediction[key]\n",
    "    #the the corresponding index for the prediction matrix\n",
    "    corr_ptid_index = X_valid_indices.index(key)\n",
    "    for timestep in timesteps_to_predict:\n",
    "    \n",
    "        prediction_entry = np.zeros((7,))\n",
    "        prediction_entry[0] = key #ptid\n",
    "        prediction_entry[1:4] = classes_to_diagnosis(predicted_diagnosis_valid[int(timestep)][corr_ptid_index])#diagnosis\n",
    "        prediction_entry[4] = predicted_adas13_valid[int(timestep)][corr_ptid_index]#ads13\n",
    "        prediction_entry[5] = predicted_vn_valid[int(timestep)][corr_ptid_index]\n",
    "        prediction_entry[6] = predicted_mmse_valid[int(timestep)][corr_ptid_index]\n",
    "        \n",
    "        final_prediction_validation[ind_valid] = prediction_entry\n",
    "        ind_valid += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concatenating Dates w/ Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatenate the original dates with the prediction matrices\n",
    "final_test_data = np.column_stack( (original_test_dates_df.values[:,0], final_prediction_test) )\n",
    "final_validation_data = np.column_stack( (original_validation_dates_df.values[:,0], final_prediction_validation) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#store predictions in dataframe\n",
    "final_test_data_df = pd.DataFrame(data=final_test_data, columns=original_test_dates_df.columns)\n",
    "final_validation_data_df = pd.DataFrame(data=final_validation_data, columns=original_validation_dates_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write predictions to a csv\n",
    "output_folder = 'results/'\n",
    "output_test = 'TADPOLE_test_predictions.csv'\n",
    "output_validation = 'TADPOLE_validation_predictions.csv'\n",
    "final_test_data_df.to_csv( (output_folder+output_test), index=False )\n",
    "final_validation_data_df.to_csv( (output_folder+output_validation), index=False )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get overall accuracy for the model\n",
    "def get_accuracy(generated_csv, given_csv):\n",
    "    my_prediction = pd.read_csv(generated_csv)\n",
    "    ground_truth = pd.read_csv(given_csv)\n",
    "    valid_index = list()\n",
    "    \n",
    "    #get list of valid indices to compare\n",
    "    for index, row in ground_truth.iterrows():\n",
    "        diagnoses = row[2:5]\n",
    "        valid_entry = pd.isnull(diagnoses)\n",
    "        if(list(valid_entry).count(False)==len(diagnoses)):\n",
    "            valid_index.append(index)\n",
    "        \n",
    "    #iterate through valid indicies and compare classification error\n",
    "    num_instances = len(valid_index)\n",
    "    num_correct = 0\n",
    "    \n",
    "    #iterate through valid indices and check if diagnoses are the same\n",
    "    for index in valid_index:\n",
    "        yhat = my_prediction.values[index]\n",
    "        y = ground_truth.values[index]\n",
    "        if( np.array_equal(yhat[2:5], y[2:5]) ):\n",
    "            num_correct+=1\n",
    "    return float(num_correct/num_instances)\n",
    "\n",
    "#calculate mse and mae for the given models\n",
    "def calculate_error(generated_csv, given_csv):\n",
    "    #load csvs\n",
    "    my_prediction = pd.read_csv(generated_csv)\n",
    "    ground_truth = pd.read_csv(given_csv)\n",
    "    \n",
    "    adas_samples = 0 #counts number of samples\n",
    "    adas_mse = 0 #accumulator variable for mse\n",
    "    adas_mae = 0 #accumulator variable for mae\n",
    "    \n",
    "    mmse_samples = 0\n",
    "    mmse_mse = 0\n",
    "    mmse_mae = 0\n",
    "    \n",
    "    vn_samples = 0\n",
    "    vn_mse = 0\n",
    "    vn_mae = 0\n",
    "    \n",
    "    #iterate through ground truth table\n",
    "    for index, row in ground_truth.iterrows():\n",
    "        #if there is a value at that positions, calculate the squared difference and absolute error\n",
    "        #and increment the samples\n",
    "        if not np.isnan(row[5]):\n",
    "            adas_mse+=(my_prediction.values[index][5] - row[5])**2\n",
    "            adas_samples+=1\n",
    "            adas_mae += abs(my_prediction.values[index][5] - row[5])\n",
    "        if not np.isnan(row[6]):\n",
    "            vn_mse += (my_prediction.values[index][6] - row[6])**2\n",
    "            vn_samples += 1\n",
    "            vn_mae += abs(my_prediction.values[index][6] - row[6])\n",
    "        if not np.isnan(row[7]):\n",
    "            mmse_mse += (my_prediction.values[index][7] - row[7])**2\n",
    "            mmse_samples += 1\n",
    "            mmse_mae += abs(my_prediction.values[index][7] - row[7])\n",
    "            \n",
    "    #divide by the number of respective samples\n",
    "    adas_mse /= adas_samples\n",
    "    vn_mse /= vn_samples\n",
    "    mmse_mse /= mmse_samples\n",
    "    \n",
    "    adas_mae /= adas_samples\n",
    "    vn_mae /= vn_samples\n",
    "    mmse_mae /= mmse_samples\n",
    "    \n",
    "    #create a list for returning\n",
    "    mse_list = [adas_mse, vn_mse, mmse_mse]\n",
    "    mae_list = [adas_mae, vn_mae, mmse_mae]\n",
    "    return mse_list, mae_list\n",
    "\n",
    "#calculate the accuracy for each diagnosis, 0 is CN, 1 is MCI and 2 is AD\n",
    "def calculate_class_accuracy(generated_csv, given_csv):\n",
    "    my_prediction = pd.read_csv(generated_csv)\n",
    "    ground_truth = pd.read_csv(given_csv)\n",
    "    \n",
    "    #create matricies for classes \n",
    "    num_right = np.array([0,0,0])\n",
    "    total_num = np.array([0,0,0])\n",
    "    \n",
    "    #iterate through ground truth dataframe and calculate accuracy for the patients that have valid diagnosis\n",
    "    for index, row in ground_truth.iterrows():\n",
    "        diagnoses = row[2:5]\n",
    "        valid_entry = pd.isnull(diagnoses)\n",
    "        \n",
    "        #if none of the entries in diagnosis are NaN, check if our prediction matches ground truth's prediction\n",
    "        if(list(valid_entry).count(False)==len(diagnoses)):\n",
    "            \n",
    "            my_current_prediction = diagnosis_to_classes(my_prediction.values[index][2:5],1)\n",
    "            actual_prediction = diagnosis_to_classes(diagnoses,1)\n",
    "            \n",
    "            #check predictions\n",
    "            if my_current_prediction == actual_prediction:                \n",
    "                num_right[int(actual_prediction)] += 1\n",
    "                total_num[int(actual_prediction)] += 1\n",
    "            else:\n",
    "                total_num[int(actual_prediction)] += 1\n",
    "            \n",
    "    return num_right/total_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = 'results/'\n",
    "output_test = 'TADPOLE_test_predictions.csv'\n",
    "output_validation = 'TADPOLE_validation_predictions.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir = 'results/'\n",
    "ground_truth_test = 'TADPOLE_TargetData_test.csv'\n",
    "ground_truth_validation = 'TADPOLE_TargetData_valid.csv'\n",
    "\n",
    "baseline_test = 'TADPOLE_baseline_test.csv'\n",
    "baseline_validation = 'TADPOLE_baseline_valid.csv'\n",
    "\n",
    "generated_test = output_test\n",
    "generated_validation = output_validation\n",
    "\n",
    "test_accuracy = get_accuracy( (results_dir + generated_test), (results_dir + ground_truth_test) )\n",
    "test_mse_loss, test_mae_loss = calculate_error( (results_dir + generated_test), (results_dir + ground_truth_test) )\n",
    "test_class_accuracy = calculate_class_accuracy((results_dir + generated_test), (results_dir + ground_truth_test))\n",
    "\n",
    "validation_accuracy = get_accuracy( (results_dir + generated_validation), (results_dir + ground_truth_validation) )\n",
    "validation_mse_loss, validation_mae_loss = calculate_error( (results_dir + generated_validation), (results_dir + ground_truth_validation) )\n",
    "validation_class_accuracy = calculate_class_accuracy((results_dir + generated_validation), (results_dir + ground_truth_validation))\n",
    "\n",
    "baseline_test_accuracy = get_accuracy( (results_dir + baseline_test), (results_dir + ground_truth_test) )\n",
    "baseline_test_mse_loss, baseline_test_mae_loss = calculate_error( (results_dir + baseline_test), (results_dir + ground_truth_test) )\n",
    "baseline_test_class_accuracy = calculate_class_accuracy( (results_dir + baseline_test), (results_dir + ground_truth_test))\n",
    "\n",
    "baseline_validation_accuracy = get_accuracy( (results_dir + baseline_validation), (results_dir + ground_truth_validation) )\n",
    "baseline_validation_mse_loss, baseline_validation_mae_loss = calculate_error( (results_dir + baseline_validation), (results_dir + ground_truth_validation) )\n",
    "baseline_validation_class_accuracy = calculate_class_accuracy((results_dir + baseline_validation), (results_dir + ground_truth_validation))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline test accuracy: 0.46397188049209137\n",
      "baseline test adas mse: 207.36759581881532\n",
      "baseline test vn mse: 0.0002267839769100788\n",
      "baseline test mmse mse: 21.59450171821306\n",
      "baseline test adas mae: 10.339721254355402\n",
      "baseline test vn mae: 0.011754191949359496\n",
      "baseline test mmse mae: 2.9072164948453607\n",
      "baseline test class accuracy [ 0.7027027   0.30735931  0.31896552]\n",
      "\n",
      "\n",
      "baseline validation accuracy: 0.33586337760910817\n",
      "baseline validation adas mse: 231.21200750469043\n",
      "baseline validation vn mse: 0.0002286186002769338\n",
      "baseline validation mmse mse: 34.47026022304833\n",
      "baseline validation adas mae: 10.671669793621014\n",
      "baseline validation vn mae: 0.01159285181294117\n",
      "baseline validation mmse mae: 3.596654275092937\n",
      "baseline validation class accuracy [ 0.49285714  0.29477612  0.24369748]\n",
      "\n",
      "\n",
      "refined test accuracy: 0.789103690685413\n",
      "refined test adas mse: 72.53164693830362\n",
      "refined test vn mse: 0.00029722749494302483\n",
      "refined test mmse mse: 8.840569578948987\n",
      "refined test adas mae: 6.031887462290427\n",
      "refined test vn mae: 0.014684586120996436\n",
      "refined test mmse mae: 2.1485181293246844\n",
      "refined test class accuracy: [ 0.86936937  0.77922078  0.65517241]\n",
      "\n",
      "\n",
      "refined validation accuracy: 0.8292220113851992\n",
      "refined validation adas mse: 91.89935767682488\n",
      "refined validation vn mse: 0.0002995567457511358\n",
      "refined validation mmse mse: 10.549923805539192\n",
      "refined validation adas mae: 6.336127035721973\n",
      "refined validation vn mae: 0.015092037362637355\n",
      "refined validation mmse mae: 2.2207991104512197\n",
      "refined validation class accuracy: [ 0.81428571  0.9141791   0.65546218]\n",
      "\n",
      "\n",
      "increase in test accuracy: 0.3251318101933216\n",
      "increase in validation accuracy: 0.49335863377609107\n"
     ]
    }
   ],
   "source": [
    "print('baseline test accuracy:', baseline_test_accuracy)\n",
    "print('baseline test adas mse:', baseline_test_mse_loss[0])\n",
    "print('baseline test vn mse:', baseline_test_mse_loss[1])\n",
    "print('baseline test mmse mse:', baseline_test_mse_loss[2])\n",
    "print('baseline test adas mae:', baseline_test_mae_loss[0])\n",
    "print('baseline test vn mae:', baseline_test_mae_loss[1])\n",
    "print('baseline test mmse mae:', baseline_test_mae_loss[2])\n",
    "print('baseline test class accuracy', baseline_test_class_accuracy)\n",
    "print('\\n')\n",
    "print('baseline validation accuracy:', baseline_validation_accuracy)\n",
    "print('baseline validation adas mse:', baseline_validation_mse_loss[0])\n",
    "print('baseline validation vn mse:', baseline_validation_mse_loss[1])\n",
    "print('baseline validation mmse mse:', baseline_validation_mse_loss[2])\n",
    "print('baseline validation adas mae:', baseline_validation_mae_loss[0])\n",
    "print('baseline validation vn mae:', baseline_validation_mae_loss[1])\n",
    "print('baseline validation mmse mae:', baseline_validation_mae_loss[2])\n",
    "print('baseline validation class accuracy', baseline_validation_class_accuracy)\n",
    "print('\\n')\n",
    "print('refined test accuracy:', test_accuracy)\n",
    "print('refined test adas mse:', test_mse_loss[0])\n",
    "print('refined test vn mse:', test_mse_loss[1])\n",
    "print('refined test mmse mse:', test_mse_loss[2])\n",
    "print('refined test adas mae:', test_mae_loss[0])\n",
    "print('refined test vn mae:', test_mae_loss[1])\n",
    "print('refined test mmse mae:', test_mae_loss[2])\n",
    "print('refined test class accuracy:', test_class_accuracy)\n",
    "print('\\n')\n",
    "print('refined validation accuracy:', validation_accuracy)\n",
    "print('refined validation adas mse:', validation_mse_loss[0])\n",
    "print('refined validation vn mse:', validation_mse_loss[1])\n",
    "print('refined validation mmse mse:', validation_mse_loss[2])\n",
    "print('refined validation adas mae:', validation_mae_loss[0])\n",
    "print('refined validation vn mae:', validation_mae_loss[1])\n",
    "print('refined validation mmse mae:', validation_mae_loss[2])\n",
    "print('refined validation class accuracy:', validation_class_accuracy)\n",
    "print('\\n')\n",
    "print('increase in test accuracy:', test_accuracy-baseline_test_accuracy)\n",
    "print('increase in validation accuracy:', validation_accuracy-baseline_validation_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_overall_tpr(generated_csv, given_csv):\n",
    "    my_prediction = pd.read_csv(generated_csv)\n",
    "    ground_truth = pd.read_csv(given_csv)\n",
    "    \n",
    "    valid_index = list()\n",
    "    \n",
    "    #get list of valid indices to compare\n",
    "    for index, row in ground_truth.iterrows():\n",
    "        diagnoses = row[2:5]\n",
    "        valid_entry = pd.isnull(diagnoses)\n",
    "        if(list(valid_entry).count(False)==len(diagnoses)):\n",
    "            valid_index.append(index)\n",
    "        \n",
    "    #iterate through valid indicies and compare classification error\n",
    "    total_samples = len(valid_index)\n",
    "    overall_tpr = []\n",
    "    overall_fpr = []\n",
    "    overall_t = 0\n",
    "    overall_f = 0\n",
    "    \n",
    "    for ind in valid_index:\n",
    "        diagnoses = ground_truth.values[ind][2:5]\n",
    "        my_current_prediction = diagnosis_to_classes(my_prediction.values[ind][2:5],1)\n",
    "        actual_prediction = diagnosis_to_classes(diagnoses,1)\n",
    "            \n",
    "        if my_current_prediction == actual_prediction:\n",
    "            overall_t += 1\n",
    "        else:\n",
    "            overall_f += 1\n",
    "            \n",
    "        overall_tpr.append(overall_t/total_samples)\n",
    "        overall_fpr.append(overall_f/total_samples)\n",
    "            \n",
    "    return overall_tpr, overall_fpr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEWCAYAAAB42tAoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nOlBoNBqNxiH/D3PNQKZuwgynAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1062d46a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#generate 50/50 plot for ROC\n",
    "coin_flip_line = []\n",
    "coin_flip_line.append(0)\n",
    "coin_flip_line.append(.5)\n",
    "coin_flip_line.append(1.0)\n",
    "\n",
    "#generate ROC over test set\n",
    "tpr_test,fpr_test = get_overall_tpr( (results_dir + generated_test), (results_dir + ground_truth_test) )\n",
    "#generate ROC over validation set\n",
    "tpr_valid, fpr_valid = get_overall_tpr( (results_dir + generated_validation), (results_dir + ground_truth_validation) )\n",
    "#generate ROC for bl over test\n",
    "bl_tpr_test, bl_fpr_test = get_overall_tpr( (results_dir + baseline_test), (results_dir + ground_truth_test) )\n",
    "#generate ROC for bl over validation\n",
    "bl_tpr_valid, bl_fpr_valid = get_overall_tpr( (results_dir + baseline_validation), (results_dir + ground_truth_validation) )\n",
    "\n",
    "\n",
    "#saturate last value so it extends the graph\n",
    "\n",
    "#baseline test\n",
    "bl_fpr_test.append(1.0)\n",
    "bl_tpr_test.append(max(bl_tpr_test))\n",
    "\n",
    "#current test\n",
    "fpr_test.append(1.0)\n",
    "tpr_test.append(max(tpr_test))\n",
    "\n",
    "#baseline valid\n",
    "bl_fpr_valid.append(1.0)\n",
    "bl_tpr_valid.append(max(bl_tpr_valid))\n",
    "\n",
    "#current valid\n",
    "fpr_valid.append(1.0)\n",
    "tpr_valid.append(max(tpr_valid))\n",
    "\n",
    "#plot roc curves\n",
    "plt.plot(coin_flip_line, coin_flip_line)\n",
    "bl_test_plot = plt.plot(bl_fpr_test, bl_tpr_test, label='Baseline Model Performance on Test Set')\n",
    "bl_validation_plot = plt.plot(bl_fpr_valid, bl_tpr_valid, label='Baseline Model Performance on Validation Set')\n",
    "test_plot = plt.plot(fpr_test, tpr_test, label='Current Model Performance on Test Set')\n",
    "validation_plot = plt.plot(fpr_valid, tpr_valid, label='Current Model Performance on Validation Set')\n",
    "\n",
    "#set up graph\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.title('ROC Curves')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

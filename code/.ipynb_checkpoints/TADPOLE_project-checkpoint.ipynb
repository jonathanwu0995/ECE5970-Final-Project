{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATAPREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PTLump(data,key):\n",
    "    ptid_roster = data['PTID_Key'].astype(int)\n",
    "    ptid_roster = ptid_roster.dropna(how='all') \n",
    "    ptid_roster = ptid_roster.unique()\n",
    "    tbl = pd.concat([pd.DataFrame(data['PTID_Key'].index.values),data['PTID_Key'].astype(int)],axis=1)\n",
    "    dic = tbl.groupby('PTID_Key').groups\n",
    "    rowIdx = []\n",
    "    for i in ptid_roster:\n",
    "        allDates = data[key].iloc[dic[i]]\n",
    "        print(i,allDates.idxmax())\n",
    "        rowIdx.append(allDates.idxmax()) \n",
    "    reducX = data.iloc[rowIdx,:]\n",
    "    reducX = reducX.reset_index(drop=True)\n",
    "    return reducX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonathanwu/anaconda3/envs/ECE5970/lib/python3.5/site-packages/IPython/core/interactiveshell.py:2698: DtypeWarning: Columns (85,86,91,92,101,102,103,104,105,106,107,108,109,456,820,1398,1643,1645,1651,1652,1881,1882,1883,1884,1885,1886,1887,1888,1889,1890,1891) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "\n",
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Importing the dataset with library pandas\n",
    "dataset = pd.read_csv('TADPOLE_InputData.csv')\n",
    "labels_train = pd.read_csv('TADPOLE_TargetData_train.csv')\n",
    "labels_test = pd.read_csv('TADPOLE_TargetData_test.csv')\n",
    "target = pd.read_csv('TADPOLE_PredictTargetData_valid.csv')\n",
    "\n",
    "# Drop meaningless rows and columns. A good practice is to drop rows before columns.\n",
    "dataset = dataset[~np.isnan(dataset['PTID_Key'])] # Drop patients with no ID, since they cannot be used for learning or prediction. [] slices the rows in dataframe.\n",
    "dataset = dataset.dropna(axis=1, how='all') # Drop empty columns in dataset \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop all columns with more than 90% of its entries as NaN\n",
    "nan_threshold = 0.8 * dataset.shape[0]\n",
    "dataset_stats = dataset.isnull().sum()\n",
    "col_indexes_to_drop = []\n",
    "for i in range(len(dataset_stats)):\n",
    "    if dataset_stats[i] > nan_threshold:\n",
    "        col_indexes_to_drop.append(i)\n",
    "dataset.drop(dataset.columns[col_indexes_to_drop],axis=1,inplace=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonathanwu/anaconda3/envs/ECE5970/lib/python3.5/site-packages/pandas/core/indexing.py:194: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n",
      "/Users/jonathanwu/anaconda3/envs/ECE5970/lib/python3.5/site-packages/ipykernel_launcher.py:75: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "#Sort all datasets by ascending patient ID.\n",
    "dataset = dataset.sort_values('PTID_Key')\n",
    "labels_train = labels_train.sort_values('PTID_Key')\n",
    "labels_test = labels_test.sort_values('PTID_Key')\n",
    "target = target.sort_values('PTID_Key')\n",
    "\n",
    "# Reindex rows\n",
    "dataset = dataset.reset_index(drop=True)\n",
    "labels_train = labels_train.reset_index(drop=True)\n",
    "labels_test = labels_test.reset_index(drop=True)\n",
    "target = target.reset_index(drop=True)\n",
    "\n",
    "# These columns are time stamps that do not carry particular meaning, so dropped for now.\n",
    "badColumns = ['update_stamp_UCSFFSL_02_01_16_UCSFFSL51ALL_08_01_16',\n",
    "    'update_stamp_UCSFFSX_11_02_15_UCSFFSX51_08_01_16']#,\n",
    "    #'update_stamp_UCBERKELEYAV45_10_17_16']#,\n",
    "    #'update_stamp_DTIROI_04_30_14']\n",
    "dataset = dataset.drop(badColumns,axis=1) # Remove this section from objDataset\n",
    "\n",
    "# Unfortunately, some numerical columns contain non-numerical character such as '>' in  '>1300'.\n",
    "# The strategy is to convert these columns to floats by extracting only numbers. For example, '>1300' goes to 1300\n",
    "columnsObjToNum = ['ABETA_UPENNBIOMK9_04_19_17','TAU_UPENNBIOMK9_04_19_17','PTAU_UPENNBIOMK9_04_19_17']#,'COMMENT_UPENNBIOMK9_04_19_17']\n",
    "#columnsObjToNum = ['PTAU_UPENNBIOMK9_04_19_17']#,'COMMENT_UPENNBIOMK9_04_19_17']\n",
    "\n",
    "for column in columnsObjToNum:\n",
    "    colIdx = dataset.columns.get_loc(column)\n",
    "    rowIdx = np.where(dataset[column].apply(type).values == str)[0] # Find all str type elements in each column, which may or may not contain non-numerical characters such as '<' or '>'.\n",
    "    for row in rowIdx: # iterate through each row of string type element in the column\n",
    "        dataset.iloc[row,colIdx] = float(re.sub(\"[^0-9.]\",\"\",dataset[column].values[row])) # Find the float/int number in the string, and cast to float type. \n",
    "dataset[columnsObjToNum] = dataset[columnsObjToNum].astype(float) # cast each column to float type\n",
    "\n",
    "# Convert date columns to date format in dataset, since they are currently imported as object columns\n",
    "for column in dataset: # variable 'column' is a string\n",
    "    if dataset[column].dtype == 'object' and dataset[column].str.match('[0-9]+/[0-9]+/[0-9]+').sum() > 0: # returns true if this column contains at least one string that matches date format.\n",
    "        dataset[column] = pd.to_datetime(dataset[column],format=\"%m/%d/%y\",errors='coerce') # convert string to date\n",
    "\n",
    "\n",
    "# Some numerical columns contain only one/few possible values, which are more likely to be categorical than numerical features.\n",
    "# As a result, such columns are converted to objective dtype. e.g. some column with only -4 and nan.\n",
    "columnsNumToCat = []\n",
    "for column in dataset:\n",
    "    psbVal = dataset[column].unique()\n",
    "    if psbVal.dtype == 'float64' and psbVal.size <= 20:\n",
    "        columnsNumToCat.append(column)\n",
    "        validRowIdx = dataset[column].notnull()\n",
    "        dataset[column].loc[validRowIdx] = dataset[column].loc[validRowIdx].astype(str)\n",
    "#dataset[columnsNumToCat] = dataset[columnsNumToCat].astype(object) # cast to object columns\n",
    "\n",
    "# Tally the data types of all data columns, and then separate them according to dtype.\n",
    "dtypeCounts = dataset.dtypes.value_counts(); # Count the number of columns for each data type. Turns out to be only 'float64' and 'object'.\n",
    "numDataset = dataset.select_dtypes(include=['float'])\n",
    "objDataset = dataset.select_dtypes(include=['object'])\n",
    "dateDataset = dataset.select_dtypes(include=['datetime64']) # select dates from objDataset for variable dateDataset\n",
    "\n",
    "# Count the number of nan's in each column to get an idea of how sparse each column is. It is very likely to drop sparse columns unless they are highly correlated to results.\n",
    "nonNanCounts_num = numDataset.count() # returns the count of non-NaN entries for each column in numDataset, since not only we want to impute, we want to know how many we impute, especially for columns with very sparse initial data.\n",
    "#temp = numDataset.count()/8715\n",
    "#temp.hist(bins=50)\n",
    "\n",
    "''' The folloing codes were used to detect the problematic columns (columnsTofix and badColumns)mentioned above\n",
    "for column in objDataset:\n",
    "    if sum(objDataset[column].apply(type) == float) -  sum(pd.isnull(objDataset[column])) > 0: # returns true if there is at least one entry that is float but not 'NaN'. 'NaN' are excluded since they are float, but not really numerical.\n",
    "        print(column)\n",
    "'''\n",
    "\n",
    "# Imputing missing data in numDataset\n",
    "from sklearn.preprocessing import Imputer\n",
    "imp = Imputer(missing_values = 'NaN', strategy = 'mean',axis = 0) # impute numerical columns\n",
    "imp = imp.fit(numDataset)\n",
    "numX = imp.transform(numDataset) # Extract data from numData as numX in dtype ndarray\n",
    "#numAttributes = numDataset.columns.values # Names of columns in numX. Executed after imputation since all NaN columns are dropped.\n",
    "\n",
    "# Encoding categorical data\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "objDataset.loc[-1,:] = np.repeat(np.nan,objDataset.shape[1]) # Append one NaN to the end of each column so that NaN must be a class for each column.\n",
    "objDataset = objDataset.fillna(value=' ') # LabelEncoder does not work with NaN, so NaN is converted to a space ' ', which is always sorted as the first class by LabelEncoder.\n",
    "le = LabelEncoder()\n",
    "catClasses = np.array([]) # initiate an empty list of attribute names\n",
    "catAttributes = np.array([]) # initiate an empty list of attribute names\n",
    "for i in range(objDataset.shape[1]):\n",
    "    objDataset.iloc[:,i] = le.fit_transform(objDataset.iloc[:,i]) # encode column i\n",
    "    classes = le.classes_ # All the labels in column i, including ' ', which was translated from NaN\n",
    "    classes[0] = 'NaN' # Replace ' ' with the attribute of the column\n",
    "    catClasses = np.append(catClasses,classes) # Append column attribute followed by all its labels to catAttribute\n",
    "    catAttributes = np.append(catAttributes, np.repeat(objDataset.columns.values[i],classes.size))\n",
    "    \n",
    "tups = [catAttributes,catClasses]\n",
    "attrTups = list(zip(*tups)) \n",
    "enc = OneHotEncoder(categorical_features = 'all') # Based on numerical categories in objDataset, encode it to one in n-class features. e.g. 0 => 0, 0, 0, ...; 1 => 0, 1, 0, ...; 2 => 0, 0, 1, 0, ...\n",
    "catDataset = pd.DataFrame(enc.fit_transform(objDataset).toarray()) # Dtype: csr_matrix => numpy array => dataframe\n",
    "catDataset = catDataset.iloc[:-1,:] # Remove the last line full of NaN added earlier\n",
    "objDataset = objDataset.iloc[:-1,:] # Remove the last line full of NaN added earlier\n",
    "multiIdx = pd.MultiIndex.from_tuples(attrTups, names = ['Attribute','Class']) # Construct a two-level column names in the format of catDataset[Attribute][Class]\n",
    "catDataset.columns = multiIdx \n",
    "\n",
    "# Convert dates to numerical, as relative days since the first date in the column for the same patient.\n",
    "# First build a dictionary between PTID_Key (patient ID) and line indices. THis will also be useful for lumping data.\n",
    "idxPTIDTable = pd.concat([pd.DataFrame(np.array(range(0,numDataset.shape[0]))),numDataset['PTID_Key']],axis=1)\n",
    "dic = idxPTIDTable.groupby('PTID_Key').groups\n",
    "\n",
    "# Each entry of dateX is the number of days since the first date of the same patient in the same column.\n",
    "dateX = np.zeros(dateDataset.shape)\n",
    "dateX[:] = np.nan\n",
    "for key, value in dic.items():\n",
    "    for i in range(dateDataset.shape[1]): # iterate through each date column\n",
    "        allDates = dateDataset.iloc[value,i] # Get all dates in column i about patient with ID 'key'\n",
    "        firstDate = allDates.min()\n",
    "        validDateIdx = np.where(~pd.isnull(allDates))[0] # indices of all non-NaT dates\n",
    "        # if not pd.isnull(firstDate): # There is at least one valid date in this column, which is the earliest date of this column\n",
    "        for j in range(len(validDateIdx)):\n",
    "            dateX[value[j],i] = (allDates[value[j]]-firstDate).days\n",
    "\n",
    "# Repacking datasets for ease of inspection\n",
    "dateDataset = pd.DataFrame(dateX, columns = dateDataset.columns.values) \n",
    "numDataset = pd.DataFrame(numX, columns = numDataset.columns.values) # numX is after imputation.\n",
    "Data = pd.concat([numDataset,catDataset], axis=1) #numDataset is imputed, catDataset does not need imputation since NaN is a class, dateDataset is not imputed.\n",
    "#XAttributes = Data.columns.values\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Work on labels_train: convert dates to number of days relative to the initial date, and then bind to the rest of data and do imputation\n",
    "idxPTIDTable_train = pd.concat([pd.DataFrame(labels_train.index.values),labels_train['PTID_Key']],axis=1)\n",
    "dic_train = idxPTIDTable_train.groupby('PTID_Key').groups\n",
    "dateY_train_raw = pd.to_datetime(labels_train['Date'],format=\"%m/%d/%y\",errors='coerce')\n",
    "dateY_train = np.zeros(labels_train['Date'].size)\n",
    "dateY_train[:] = np.nan\n",
    "for key, value in dic_train.items():\n",
    "    allDates = dateY_train_raw[value] # Get all dates in column i about patient with ID 'key'\n",
    "    firstDate = allDates.min()\n",
    "    validDateIdx = np.where(~pd.isnull(allDates))[0]\n",
    "    for j in range(len(validDateIdx)):\n",
    "        dateY_train[value[j]] = (allDates[value[j]]-firstDate).days\n",
    "labels_train['Date'] = dateY_train\n",
    "\n",
    "# On Training labels: Impute missing data based on most frequent value for each individual patient\n",
    "imp = Imputer(missing_values = 'NaN', strategy = 'most_frequent', axis = 0)\n",
    "global_means = labels_train.iloc[:,2:].mean() # will be used when the whole column is missing\n",
    "for key, value in dic_train.items():\n",
    "        # Get the most frequent value in each column of each patient\n",
    "        subLbl = labels_train.iloc[value,:]\n",
    "        modes = subLbl.mode().iloc[0,:]\n",
    "        diag = subLbl.iloc[:,2]*4+subLbl.iloc[:,3]*2+subLbl.iloc[:,4]\n",
    "        diag_mode = diag.mode()\n",
    "        # If the most frequent element of a column is a nan, that means all elemetns are nan in that column, so we will have to impute missing data in that column from input X.\n",
    "        if diag_mode.size == 0: # If there is no diagnostic result, impute with the most typical situation\n",
    "            modes['CN_Diag'] = 0\n",
    "            modes['MCI_Diag'] = 1\n",
    "            modes['AD_Diag'] = 0 \n",
    "        else:\n",
    "            b = bin(int(diag_mode.iloc[0]))[2:].zfill(3)\n",
    "            modes['CN_Diag'] = b[0]\n",
    "            modes['MCI_Diag'] = b[1]\n",
    "            modes['AD_Diag'] = b[2] \n",
    "    \n",
    "        if np.isnan(modes['ADAS13']):\n",
    "            modes['ADAS13'] = global_means['ADAS13']\n",
    "        if np.isnan(modes['Ventricles_Norm']):\n",
    "            modes['Ventricles_Norm'] = global_means['Ventricles_Norm']\n",
    "        if np.isnan(modes['MMSE']):\n",
    "            modes['MMSE'] = global_means['MMSE']\n",
    "\n",
    "        # impute missing data with either the most frequent item in this column for this patient or population mode if no record is found for this patient.    \n",
    "        y_train_individual = labels_train.iloc[value,:].append(modes)\n",
    "        imp = imp.fit(y_train_individual.values[:,2:]) \n",
    "        temp = imp.transform(y_train_individual.values[:,2:])\n",
    "        labels_train.iloc[value,2:] = temp[:-1,:]\n",
    "\n",
    "colNames = labels_train.columns.tolist()\n",
    "newColNames = [colNames[1],colNames[0]]+ colNames[2:]\n",
    "labels_train = labels_train[newColNames]\n",
    "y_train = labels_train.values\n",
    "yAttributes = labels_train.columns.values\n",
    "\n",
    "# Work on labels_test: convert dates to number of days relative to the initial date, and then bind to the rest of data and do imputation\n",
    "idxPTIDTable_test = pd.concat([pd.DataFrame(labels_test.index.values),labels_test['PTID_Key']],axis=1)\n",
    "dic_test = idxPTIDTable_test.groupby('PTID_Key').groups\n",
    "dateY_test_raw = pd.to_datetime(labels_test['Date'],format=\"%Y-%m-%d\",errors='coerce')\n",
    "dateY_test = np.zeros(dateY_test_raw.size)\n",
    "dateY_test[:] = np.nan\n",
    "for key, value in dic_test.items():\n",
    "    allDates = dateY_test_raw[value] # Get all dates in column i about patient with ID 'key'\n",
    "    firstDate = allDates.min()\n",
    "    validDateIdx = np.where(~pd.isnull(allDates))[0]\n",
    "    for j in range(len(validDateIdx)):\n",
    "        dateY_test[value[j]] = (allDates[value[j]]-firstDate).days\n",
    "labels_test['Date'] = dateY_test\n",
    "colNames = labels_test.columns.tolist()\n",
    "newColNames = [colNames[1],colNames[0]]+ colNames[2:] # switch PTID_Key to the first column\n",
    "labels_test = labels_test[newColNames]\n",
    "\n",
    "# Build a row index lookup table for each patient from Data to labels_train/labels_test\n",
    "ptid_roster = Data['PTID_Key']\n",
    "ptid_roster = ptid_roster.dropna(how='all') \n",
    "ptid_roster = ptid_roster.unique()\n",
    "ptid_roster.sort()\n",
    "ptid_roster = ptid_roster.astype(int)\n",
    "\n",
    "#train data\n",
    "ptid_train = labels_train['PTID_Key'].unique()\n",
    "ptid_train.sort()\n",
    "#test data\n",
    "ptid_test = labels_test['PTID_Key'].unique()\n",
    "ptid_test.sort()\n",
    "#validation data\n",
    "ptid_validation = target['PTID_Key'].unique()\n",
    "ptid_validation.sort()\n",
    "\n",
    "\n",
    "# Separate input data for traning and test sets \n",
    "X_train_rowIdx = []\n",
    "for i in ptid_train:\n",
    "    X_train_rowIdx.extend(dic[i].tolist())\n",
    "    \n",
    "X_test_rowIdx = []\n",
    "for i in ptid_test:\n",
    "    X_test_rowIdx.extend(dic[i].tolist())\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "X_train = Data.iloc[X_train_rowIdx,:]\n",
    "X_train = X_train.reset_index(drop=True)\n",
    "X_test = Data.iloc[X_test_rowIdx,:]\n",
    "X_test = X_test.reset_index(drop=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get last patient visit from input data\n",
    "patient_visit = {}\n",
    "for index, row in dataset.iterrows():\n",
    "#for i in range(len(dataset.values)):\n",
    "    if row[0] not in patient_visit.keys():\n",
    "        patient_visit[row[0]] = list()\n",
    "    patient_visit[row[0]].append(row[1])\n",
    "\n",
    "first_patient_visit = {}    \n",
    "for key, val in patient_visit.items():\n",
    "    first_patient_visit[key] = max(patient_visit[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "og_train = pd.read_csv('TADPOLE_TargetData_train.csv')\n",
    "og_test = pd.read_csv('TADPOLE_TargetData_test.csv')\n",
    "target = pd.read_csv('TADPOLE_PredictTargetData_valid.csv')\n",
    "\n",
    "#Sort all datasets by ascending patient ID.\n",
    "og_train = og_train.sort_values('PTID_Key')\n",
    "og_test = og_test.sort_values('PTID_Key')\n",
    "\n",
    "# Reindex rows\n",
    "og_train = og_train.reset_index(drop=True)\n",
    "og_test = og_test.reset_index(drop=True)\n",
    "\n",
    "og_train['Date'] = pd.to_datetime(og_train['Date'],format=\"%m/%d/%y\",errors='coerce')\n",
    "#pd.to_datetime(labels_test['Date'],format=\"%Y-%m-%d\",errors='coerce')\n",
    "og_test['Date'] = pd.to_datetime(og_test['Date'],format=\"%Y-%m-%d\",errors='coerce')\n",
    "target['Date'] = pd.to_datetime(target['Date'],format=\"%m/%d/%y\",errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert dates in train, test and target to absolute years passed from first visit\n",
    "abs_days_train = np.zeros((og_train.shape[0],))\n",
    "for index, row in og_train.iterrows():\n",
    "    current_ptid = row[1]\n",
    "    abs_day_conv = row[0] - first_patient_visit[current_ptid]\n",
    "    #abs_days_train[index] = abs_day_conv.days/360\n",
    "    abs_days_train[index] = float(abs_day_conv.days/360) - 0.3\n",
    "    abs_days_train[index] = abs(round(abs_days_train[index]))\n",
    "og_train['Date'] = abs_days_train\n",
    "\n",
    "abs_days_test = np.zeros((og_test.shape[0],))\n",
    "for index, row in og_test.iterrows():\n",
    "    current_ptid = row[1]\n",
    "    abs_day_conv = row[0] - first_patient_visit[current_ptid]\n",
    "    #abs_days_test[index] = abs_day_conv.days/360\n",
    "    abs_days_test[index] = float(abs_day_conv.days/360) - 0.3\n",
    "    abs_days_test[index] = abs(round(abs_days_test[index]))\n",
    "og_test['Date'] = abs_days_test\n",
    "\n",
    "abs_days_target = np.zeros((target.shape[0],))\n",
    "for index, row in target.iterrows():\n",
    "    current_ptid = row[1]\n",
    "    abs_day_conv = row[0] - first_patient_visit[current_ptid]\n",
    "    #abs_days_target[index] = abs_day_conv.days/360\n",
    "    abs_days_target[index] = float(abs_day_conv.days/360) - 0.3\n",
    "    abs_days_target[index] = abs(round(abs_days_target[index]))\n",
    "target['Date'] = abs_days_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make dates relative to input data\n",
    "labels_train['Date'] = og_train['Date']\n",
    "labels_test['Date'] = og_test['Date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sort years and space them\n",
    "\n",
    "#load dictionary key -> y_feats\n",
    "y_train_dict_years = {}\n",
    "for index, row in labels_train.iterrows():\n",
    "    current_ptid = row[0]\n",
    "    if current_ptid not in y_train_dict_years.keys():\n",
    "        y_train_dict_years[current_ptid] = list()\n",
    "    y_train_dict_years[current_ptid].append(row)\n",
    "\n",
    "time_sorted_y_train = {}\n",
    "for key, val in y_train_dict_years.items():\n",
    "    #print(val[0][1])\n",
    "    time_sorted_y_train[key] = list()\n",
    "    y_time_list = []\n",
    "    vals_to_add = len(val)\n",
    "    for i in range(len(val)):\n",
    "        y_time_list.append(val[i][1])\n",
    "    y_time_list.sort()\n",
    "    #infinite while loop to sort bruteforce\n",
    "    index = 0\n",
    "    while(index != vals_to_add):\n",
    "        for i in range(len(val)):\n",
    "            if val[i][1] == y_time_list[index]:\n",
    "                time_sorted_y_train[key].append(val[i])\n",
    "                break\n",
    "        index+=1   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert train df to time series, ptid_train has all unique \n",
    "y_train_num_samples = len(ptid_train)\n",
    "y_train_cats = list(labels_train.columns)\n",
    "y_train_cats.remove('Date')\n",
    "y_train_time_shifted_cats = list()\n",
    "for cats in y_train_cats:\n",
    "    if cats == 'PTID_Key':\n",
    "        y_train_time_shifted_cats.append(cats)\n",
    "    else:\n",
    "        y_train_time_shifted_cats.append(str(cats) + '_t')\n",
    "        y_train_time_shifted_cats.append(str(cats) + '_t+1')\n",
    "        y_train_time_shifted_cats.append(str(cats) + '_t+2')\n",
    "        y_train_time_shifted_cats.append(str(cats) + '_t+3')\n",
    "        y_train_time_shifted_cats.append(str(cats) + '_t+4')\n",
    "        y_train_time_shifted_cats.append(str(cats) + '_t+5')\n",
    "y_train_num_feats = len(y_train_time_shifted_cats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make dictionary to map ptid -> all rows\n",
    "y_train_ptid = {}\n",
    "for index, row in labels_train.iterrows():\n",
    "    if row[0] not in y_train_ptid.keys():\n",
    "        y_train_ptid[row[0]] = list()\n",
    "    y_train_ptid[row[0]].append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make dictionary that maps ptid -> diagnoses at all timesteps\n",
    "y_train_timestep = {}\n",
    "for key, val in y_train_ptid.items():\n",
    "    y_train_timestep[key] = {}\n",
    "    #y_train_timestep[key]['Last_TS'] = 0\n",
    "    y_train_timestep[key]['timestep'] = list()\n",
    "    y_train_timestep[key]['TS'] = [[],[],[],[],[],[]]\n",
    "    for entry in val:\n",
    "        #get timestep\n",
    "        if int(entry[1]) not in y_train_timestep[key]['timestep']:\n",
    "            y_train_timestep[key]['timestep'].append(entry[1])\n",
    "        #if int(entry[1]) > y_train_timestep[key]['Last_TS']:\n",
    "            #y_train_timestep[key]['Last_TS'] = entry[1]\n",
    "        y_train_timestep[key]['TS'][int(entry[1])].append(entry)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sort entries by timesteps\n",
    "y_timestep_dict = {}\n",
    "y_timestep_dict[0] = list()\n",
    "y_timestep_dict[1] = list()\n",
    "y_timestep_dict[2] = list()\n",
    "y_timestep_dict[3] = list()\n",
    "y_timestep_dict[4] = list()\n",
    "y_timestep_dict[5] = list()\n",
    "\n",
    "for index, row in labels_train.iterrows():\n",
    "    key = int(row[1])\n",
    "    y_timestep_dict[key].append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "condensed_imputed_y_train = {}\n",
    "for key, val in y_train_timestep.items():\n",
    "    condensed_imputed_y_train[key] = np.zeros((6,8))\n",
    "    last_ts = val['timestep']\n",
    "    all_ts = val['TS']\n",
    "    #print(len(all_ts))\n",
    "    #iterate through list of lists\n",
    "    for i in range(len(all_ts)):\n",
    "        total_entry = len(all_ts[i])\n",
    "        \n",
    "        #if there is no entry for the year\n",
    "        if total_entry == 0:\n",
    "            condensed_entry = np.zeros((8,))\n",
    "            #population mean for target variables\n",
    "            #last diagnosis patient got/population mode for diagnosis at that timestep\n",
    "            #years_to_impute = list( set([0,1,2,3,4,5]) - set(y_train_timestep[i]['timestep']) )\n",
    "            \n",
    "            #use global means at the timestep to get global averages to impute missing timesteps for patients\n",
    "            timestep_entries = y_timestep_dict[i]\n",
    "            running_adas = 0\n",
    "            running_vn = 0\n",
    "            running_mmse = 0\n",
    "            #future_ts_diagnoses = [0,0,0]\n",
    "            \n",
    "            for entry in timestep_entries:\n",
    "                running_adas += entry[5]\n",
    "                running_vn += entry[6]\n",
    "                running_mmse += entry[7]\n",
    "            running_adas /= len(timestep_entries)\n",
    "            running_vn /= len(timestep_entries)\n",
    "            running_mmse /= len(timestep_entries)\n",
    "            condensed_entry[5] = running_adas\n",
    "            condensed_entry[6] = running_vn\n",
    "            condensed_entry[7] = running_mmse\n",
    "            \n",
    "            #get last year logged\n",
    "            patient_last_year = max(y_train_timestep[key]['timestep'])\n",
    "            #if there are missing intermediate values\n",
    "            if i < patient_last_year:\n",
    "                ptid_last_visit_entry = all_ts[int(patient_last_year)]\n",
    "                condensed_entry[2:5] = ptid_last_visit_entry[0][2:5]\n",
    "                condensed_entry[0] = key\n",
    "                condensed_entry[1] = i\n",
    "                #get global means averaged value for patient's vn, \n",
    "            #tiestep is from a year after\n",
    "            else:\n",
    "                condensed_entry[0] = key\n",
    "                condensed_entry[1] = i\n",
    "                ptid_last_visit_entry = all_ts[int(patient_last_year)]\n",
    "                condensed_entry[2:5] = ptid_last_visit_entry[0][2:5]\n",
    "        #if theres more than one entry for the year, average over the \n",
    "        elif total_entry > 0:\n",
    "            condensed_entry = np.zeros((8,))\n",
    "            condensed_entry[0] = key\n",
    "            condensed_entry[1] = i\n",
    "            condensed_entry[2:5] = all_ts[i][0][2:5]\n",
    "            running_adas = 0\n",
    "            running_vn = 0\n",
    "            running_mmse = 0\n",
    "\n",
    "            for entry in all_ts[i]:\n",
    "                running_adas += entry[5]\n",
    "                running_vn += entry[6]\n",
    "                running_mmse += entry[7]\n",
    "\n",
    "            running_adas /= total_entry\n",
    "            running_vn /= total_entry\n",
    "            running_mmse /= total_entry\n",
    "            condensed_entry[5] = running_adas\n",
    "            condensed_entry[6] = running_vn\n",
    "            condensed_entry[7] = running_mmse\n",
    "        #if theres no entry for the year, use population mean for target variables & last diagnosis patient\n",
    "        # for values in between\n",
    "        elif total_entry == 1:\n",
    "            condensed_entry = np.zeros((8,))\n",
    "            condensed_entry = all_ts[i][0]\n",
    "                    \n",
    "        condensed_imputed_y_train[key][i] = condensed_entry\n",
    "                    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_time_series = np.zeros((y_train_num_samples, y_train_num_feats))\n",
    "y_train_ts_ind = 0\n",
    "for key, val in condensed_imputed_y_train.items():\n",
    "    set_key = 0\n",
    "    #print(key)\n",
    "    #iterate through entries per key\n",
    "    y_train_entry_array = np.zeros((y_train_num_feats,))\n",
    "    for entry in val:\n",
    "        current_timestep = entry[1]\n",
    "        entry_cn = entry[2]\n",
    "        entry_mci = entry[3]\n",
    "        entry_ad = entry[4]\n",
    "        entry_adas = entry[5]\n",
    "        entry_vn = entry[6]\n",
    "        entry_mmse = entry[7]\n",
    "        #set patient id\n",
    "        if set_key == 0:\n",
    "            y_train_entry_array[0] = entry[0]\n",
    "            set_key = 1\n",
    "        if current_timestep==0:\n",
    "            ts_offset = 1\n",
    "            y_train_entry_array[1] = entry_cn\n",
    "            y_train_entry_array[7] = entry_mci\n",
    "            y_train_entry_array[13] = entry_ad\n",
    "            y_train_entry_array[19] = entry_adas\n",
    "            y_train_entry_array[25] = entry_vn\n",
    "            y_train_entry_array[31] = entry_mmse\n",
    "        elif current_timestep==1:\n",
    "            y_train_entry_array[2] = entry_cn\n",
    "            y_train_entry_array[8] = entry_mci\n",
    "            y_train_entry_array[14] = entry_ad\n",
    "            y_train_entry_array[20] = entry_adas\n",
    "            y_train_entry_array[26] = entry_vn\n",
    "            y_train_entry_array[32] = entry_mmse            \n",
    "\n",
    "        elif current_timestep==2:\n",
    "            y_train_entry_array[3] = entry_cn\n",
    "            y_train_entry_array[9] = entry_mci\n",
    "            y_train_entry_array[15] = entry_ad\n",
    "            y_train_entry_array[21] = entry_adas\n",
    "            y_train_entry_array[27] = entry_vn\n",
    "            y_train_entry_array[33] = entry_mmse\n",
    "        elif current_timestep==3:\n",
    "            y_train_entry_array[4] = entry_cn\n",
    "            y_train_entry_array[10] = entry_mci\n",
    "            y_train_entry_array[16] = entry_ad\n",
    "            y_train_entry_array[22] = entry_adas\n",
    "            y_train_entry_array[28] = entry_vn\n",
    "            y_train_entry_array[34] = entry_mmse\n",
    "        elif current_timestep==4:\n",
    "            y_train_entry_array[5] = entry_cn\n",
    "            y_train_entry_array[11] = entry_mci\n",
    "            y_train_entry_array[17] = entry_ad\n",
    "            y_train_entry_array[23] = entry_adas\n",
    "            y_train_entry_array[29] = entry_vn\n",
    "            y_train_entry_array[35] = entry_mmse\n",
    "        elif current_timestep==5:\n",
    "            y_train_entry_array[6] = entry_cn\n",
    "            y_train_entry_array[12] = entry_mci\n",
    "            y_train_entry_array[18] = entry_ad\n",
    "            y_train_entry_array[24] = entry_adas\n",
    "            y_train_entry_array[30] = entry_vn\n",
    "            y_train_entry_array[36] = entry_mmse \n",
    "    y_train_time_series[y_train_ts_ind] = y_train_entry_array\n",
    "    y_train_ts_ind +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_ts_df = pd.DataFrame(data=y_train_time_series, columns=y_train_time_shifted_cats)\n",
    "\n",
    "#Sort all datasets by ascending patient ID.\n",
    "y_train_ts_df = y_train_ts_df.sort_values('PTID_Key')\n",
    "\n",
    "# Reindex rows\n",
    "y_train_ts_df = y_train_ts_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CREATE TIME SERIES INPUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mod_numDataset = numDataset.insert(1, 'EXAMDATE', dateDataset.values[:,0])\n",
    "time_series_data = Data.insert(1, 'Elapsed Days', dateX[:,0] )\n",
    "#time_series_data = Data.insert(0, 'Elapsed Days', dateX[:,0] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create PTID Dictionary, maps PTID -> all visits\n",
    "PTID_Dict = {}\n",
    "for i in range(len(Data)):\n",
    "    if Data.values[i][0] not in PTID_Dict.keys():\n",
    "        PTID_Dict[Data.values[i][0]] = list()\n",
    "        PTID_Dict[Data.values[i][0]].append(Data.values[i])\n",
    "    else:\n",
    "        PTID_Dict[Data.values[i][0]].append(Data.values[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#sort PTID entries by time of visit, val is a list of patient visits\n",
    "time_sorted_PTID_dict = {}\n",
    "for key, val in PTID_Dict.items():\n",
    "    time_sorted_PTID_dict[key] = list()\n",
    "    time_list = []\n",
    "    vals_to_add = len(val)\n",
    "    for i in range(len(val)):\n",
    "        time_list.append(val[i][1])\n",
    "    time_list.sort()\n",
    "    #infinite while loop to sort bruteforce\n",
    "    index = 0\n",
    "    while(index != vals_to_add):\n",
    "        for i in range(len(val)):\n",
    "            if val[i][1] == time_list[index]:\n",
    "                time_sorted_PTID_dict[key].append(val[i])\n",
    "                break\n",
    "        index+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert time elapsed from days -> years\n",
    "year_ptid_dict={}\n",
    "for key,val in time_sorted_PTID_dict.items():\n",
    "    year_ptid_dict[key] = list()\n",
    "    for i in range(len(val)):  \n",
    "        year_ptid_dict[key].append(val[i])\n",
    "        year_conv = val[i][1]/360\n",
    "        year_ptid_dict[key][i][1] = year_conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get visits that are spaced a year apart\n",
    "import math\n",
    "condensed_time_series_dict = {}\n",
    "for key, val in year_ptid_dict.items():\n",
    "    condensed_time_series_dict[key] = list()\n",
    "    last_added = 0\n",
    "    condensed_time_series_dict[key].append(val[0])\n",
    "    if len(val)>1:\n",
    "        for i in range(len(val)-1):\n",
    "            tmp = val[i+1][1]-.3 #var to check if year is at least .85\n",
    "            if (round(tmp)) >= (last_added + 1):\n",
    "                condensed_time_series_dict[key].append(val[i+1])\n",
    "                last_added = round(tmp)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get last two visits of each patient \n",
    "last_two_visits_time_series_dict = {}\n",
    "for key, val in condensed_time_series_dict.items():\n",
    "    last_two_visits_time_series_dict[key]=list()\n",
    "    for entry in val[-2:]: \n",
    "        last_two_visits_time_series_dict[key].append(np.delete(entry, 1))\n",
    "    if len(last_two_visits_time_series_dict[key])==1:\n",
    "        last_two_visits_time_series_dict[key].append(np.delete(val,1))\n",
    "        \n",
    "#pack dictionary into dataframe for inspection\n",
    "last_two_visits_df = pd.DataFrame(data=last_two_visits_time_series_dict)\n",
    "last_two_visits_df = last_two_visits_df.transpose()\n",
    "last_two_visits_df.columns = ['t-1', 't']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#last_two_visits_df[i][j], where i = ptid-1, j=time step where j=0=(t-1), j=1=t\n",
    "num_ptids = last_two_visits_df.shape[0]\n",
    "num_feats = Data.shape[1]\n",
    "\n",
    "\n",
    "t_1_ptid_feats = np.zeros((num_ptids,num_feats-1))\n",
    "t_ptid_feats = np.zeros((num_ptids,num_feats-1))\n",
    "\n",
    "\n",
    "for key, val in last_two_visits_time_series_dict.items():\n",
    "    ind = int(key-1)\n",
    "    t_1_ptid_feats[ind]=val[0]\n",
    "    t_ptid_feats[ind] = val[1]\n",
    "\n",
    "cats = list(Data.columns)\n",
    "cats.remove('Elapsed Days')\n",
    "\n",
    "time_shifted_cats = []\n",
    "for i in range(len(cats)):\n",
    "    if cats[i] == 'PTID_Key':\n",
    "        time_shifted_cats.append(cats[i])\n",
    "    else:\n",
    "        time_shifted_cats.append(str(cats[i])+'_t-1')\n",
    "        time_shifted_cats.append(str(cats[i])+'_t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODIFY INPUT VECTOR ACCORDING TO TRAIN, TEST AND "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_shifted_feat_matrix = np.zeros((num_ptids, len(time_shifted_cats)))\n",
    "for i in range(len(dataset.columns)):\n",
    "    if i==0:\n",
    "        time_shifted_feat_matrix[:,i] = t_ptid_feats[:,i]\n",
    "    else:\n",
    "        #t-1\n",
    "        time_shifted_feat_matrix[:,2*i-1] = t_1_ptid_feats[:,i]\n",
    "        #t\n",
    "        time_shifted_feat_matrix[:,2*i] = t_ptid_feats[:,i]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "#modify input for training\n",
    "Xts_train_data = np.zeros((len(ptid_train), len(time_shifted_cats)))\n",
    "for i in range(len(ptid_train)):\n",
    "    ptid_index = ptid_train[i]-1\n",
    "    Xts_train_data[i] = time_shifted_feat_matrix[ptid_index]\n",
    "Xts_train_df = pd.DataFrame(data=Xts_train_data, columns=time_shifted_cats)\n",
    "\n",
    "#modify input for testing\n",
    "Xts_test_data = np.zeros((len(ptid_test),len(time_shifted_cats)))\n",
    "for i in range(len(ptid_test)):\n",
    "    ptid_index = ptid_test[i]-1\n",
    "    Xts_test_data[i] = time_shifted_feat_matrix[ptid_index]\n",
    "Xts_test_df = pd.DataFrame(data=Xts_test_data, columns=time_shifted_cats)\n",
    "\n",
    "#modify input for validation\n",
    "Xts_target_data = np.zeros((len(ptid_validation), len(time_shifted_cats)))\n",
    "for i in range(len(ptid_validation)):\n",
    "    ptid_index = ptid_validation[i]-1\n",
    "    Xts_target_data[i] = time_shifted_feat_matrix[ptid_index]\n",
    "Xts_target_df = pd.DataFrame(data=Xts_target_data, columns=time_shifted_cats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc = StandardScaler()\n",
    "X_train_scaled = sc.fit_transform(Xts_train_df.values[:,1:])\n",
    "X_test_scaled = sc.fit_transform(Xts_test_df.values[:,1:])\n",
    "X_validation_scaled = sc.fit_transform(Xts_target_df.values[:,1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Support Vector Classifier/Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create ptid -> years for test and validation set\n",
    "test_ptid_prediction = {}\n",
    "target_ptid_prediction = {}\n",
    "for index, row in labels_test.iterrows():\n",
    "    if row[0] not in test_ptid_prediction.keys():\n",
    "        test_ptid_prediction[row[0]] = list()\n",
    "    test_ptid_prediction[row[0]].append(row[1]) #add year to prediction\n",
    "\n",
    "for index, row in target.iterrows():\n",
    "    if row[1] not in target_ptid_prediction.keys():\n",
    "        target_ptid_prediction[row[1]] = list()\n",
    "    target_ptid_prediction[row[1]].append(row[0]) #add year to prediction\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### MMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVR(C=100.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma='auto',\n",
       "  kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "mmse_models = [svm.SVR(C=1e2),svm.SVR(C=1e2),svm.SVR(C=1e2),svm.SVR(C=1e2),svm.SVR(C=1e2),svm.SVR(C=1e2)]\n",
    "y_mmse_t = y_train_ts_df.values[:,31]\n",
    "y_mmse_tp1 = y_train_ts_df.values[:,32]\n",
    "y_mmse_tp2 = y_train_ts_df.values[:,33]\n",
    "y_mmse_tp3 = y_train_ts_df.values[:,34]\n",
    "y_mmse_tp4 = y_train_ts_df.values[:,35]\n",
    "y_mmse_tp5 = y_train_ts_df.values[:,36]\n",
    "\n",
    "mmse_models[0].fit(X_train_scaled, y_mmse_t)\n",
    "mmse_models[1].fit(X_train_scaled, y_mmse_tp1)\n",
    "mmse_models[2].fit(X_train_scaled, y_mmse_tp2)\n",
    "mmse_models[3].fit(X_train_scaled, y_mmse_tp3)\n",
    "mmse_models[4].fit(X_train_scaled, y_mmse_tp4)\n",
    "mmse_models[5].fit(X_train_scaled, y_mmse_tp5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Ventricle Norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma='auto',\n",
       "  kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "vn_models = [svm.SVR(),svm.SVR(),svm.SVR(),svm.SVR(),svm.SVR(),svm.SVR()]\n",
    "y_vn_t = y_train_ts_df.values[:,25]\n",
    "y_vn_tp1 = y_train_ts_df.values[:,26]\n",
    "y_vn_tp2 = y_train_ts_df.values[:,27]\n",
    "y_vn_tp3 = y_train_ts_df.values[:,28]\n",
    "y_vn_tp4 = y_train_ts_df.values[:,29]\n",
    "y_vn_tp5 = y_train_ts_df.values[:,30]\n",
    "\n",
    "vn_models[0].fit(X_train_scaled, y_vn_t)\n",
    "vn_models[1].fit(X_train_scaled, y_vn_tp1)\n",
    "vn_models[2].fit(X_train_scaled, y_vn_tp2)\n",
    "vn_models[3].fit(X_train_scaled, y_vn_tp3)\n",
    "vn_models[4].fit(X_train_scaled, y_vn_tp4)\n",
    "vn_models[5].fit(X_train_scaled, y_vn_tp5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Diagnosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diagnosis_to_classes(y_diagnosis_cols, num_entries):\n",
    "    condensed_classes = np.zeros((num_entries,))\n",
    "    if num_entries==1:\n",
    "        check_equivalence_healthy = y_diagnosis_cols == np.array([1,0,0])\n",
    "        check_equivalence_mci = y_diagnosis_cols == np.array([0,1,0])\n",
    "        check_equivalence_ad = y_diagnosis_cols == np.array([0,0,1])\n",
    "        if check_equivalence_healthy.all() == True:\n",
    "            condensed_classes = 0\n",
    "        elif check_equivalence_mci.all() == True:\n",
    "            condensed_classes = 1\n",
    "        elif check_equivalence_ad.all() == True:\n",
    "            condensed_classes = 2    \n",
    "    else:\n",
    "        for i in range(len(condensed_classes)):\n",
    "            check_equivalence_healthy = y_diagnosis_cols[i] == np.array([1,0,0])\n",
    "            check_equivalence_mci = y_diagnosis_cols[i] == np.array([0,1,0])\n",
    "            check_equivalence_ad = y_diagnosis_cols[i] == np.array([0,0,1])\n",
    "            if check_equivalence_healthy.all() == True:\n",
    "                condensed_classes[i] = 0\n",
    "            elif check_equivalence_mci.all() == True:\n",
    "                condensed_classes[i] = 1\n",
    "            elif check_equivalence_ad.all() == True:\n",
    "                condensed_classes[i] = 2        \n",
    "    return condensed_classes\n",
    "\n",
    "\n",
    "def classes_to_diagnosis(y_diagnosis_col):\n",
    "    condensed_classes = np.zeros((1, 3))\n",
    "    if y_diagnosis_col == 0:\n",
    "        condensed_classes = np.array([1,0,0])\n",
    "    elif y_diagnosis_col == 1:\n",
    "        condensed_classes = np.array([0,1,0])\n",
    "    elif y_diagnosis_col == 2:\n",
    "        condensed_classes = np.array([0,0,1])     \n",
    "    return condensed_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.  0.]\n"
     ]
    }
   ],
   "source": [
    "diags = np.zeros((2,3))\n",
    "diags[0] = [0,1,0]\n",
    "diags[1] = [1,0,0]\n",
    "print(diagnosis_to_classes(diags,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=2000, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diagnosis_models = [svm.SVC(max_iter=2000), svm.SVC(max_iter=2000), svm.SVC(max_iter=2000), svm.SVC(max_iter=2000), svm.SVC(max_iter=2000), svm.SVC(max_iter=2000)]\n",
    "y_diag_t = np.column_stack( (y_train_ts_df.values[:,1], y_train_ts_df.values[:,7], y_train_ts_df.values[:,13]) )\n",
    "y_diag_tp1 = np.column_stack( (y_train_ts_df.values[:,2], y_train_ts_df.values[:,8], y_train_ts_df.values[:,14]) )\n",
    "y_diag_tp2 = np.column_stack( (y_train_ts_df.values[:,3], y_train_ts_df.values[:,9], y_train_ts_df.values[:,15]) )\n",
    "y_diag_tp3 = np.column_stack( (y_train_ts_df.values[:,4], y_train_ts_df.values[:,10], y_train_ts_df.values[:,16]) )\n",
    "y_diag_tp4 = np.column_stack( (y_train_ts_df.values[:,5], y_train_ts_df.values[:,11], y_train_ts_df.values[:,17]) )\n",
    "y_diag_tp5 = np.column_stack( (y_train_ts_df.values[:,6], y_train_ts_df.values[:,12], y_train_ts_df.values[:,18]) )\n",
    "\n",
    "y_diag_t_classes = diagnosis_to_classes(y_diag_t, y_diag_t.shape[0])\n",
    "y_diag_tp1_classes = diagnosis_to_classes(y_diag_tp1, y_diag_tp1.shape[0])\n",
    "y_diag_tp2_classes = diagnosis_to_classes(y_diag_tp2, y_diag_tp2.shape[0])\n",
    "y_diag_tp3_classes = diagnosis_to_classes(y_diag_tp3, y_diag_tp3.shape[0])\n",
    "y_diag_tp4_classes = diagnosis_to_classes(y_diag_tp4, y_diag_tp4.shape[0])\n",
    "y_diag_tp5_classes = diagnosis_to_classes(y_diag_tp5, y_diag_tp5.shape[0])\n",
    "\n",
    "diagnosis_models[0].fit(X_train_scaled, y_diag_t_classes)\n",
    "diagnosis_models[1].fit(X_train_scaled, y_diag_tp1_classes)\n",
    "diagnosis_models[2].fit(X_train_scaled, y_diag_tp2_classes)\n",
    "diagnosis_models[3].fit(X_train_scaled, y_diag_tp3_classes)\n",
    "diagnosis_models[4].fit(X_train_scaled, y_diag_tp4_classes)\n",
    "diagnosis_models[5].fit(X_train_scaled, y_diag_tp5_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### ADAS13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVR(C=100000000.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1,\n",
       "  gamma='auto', kernel='rbf', max_iter=-1, shrinking=True, tol=0.001,\n",
       "  verbose=False)"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adas13_models = [svm.SVR(C=1e8),svm.SVR(C=1e8),svm.SVR(C=1e8),svm.SVR(C=1e8),svm.SVR(C=1e8),svm.SVR(C=1e8)]\n",
    "y_adas13_t = y_train_ts_df.values[:,19]\n",
    "y_adas13_tp1 = y_train_ts_df.values[:,20]\n",
    "y_adas13_tp2 = y_train_ts_df.values[:,21]\n",
    "y_adas13_tp3 = y_train_ts_df.values[:,22]\n",
    "y_adas13_tp4 = y_train_ts_df.values[:,23]\n",
    "y_adas13_tp5 = y_train_ts_df.values[:,24]\n",
    "\n",
    "adas13_models[0].fit(X_train_scaled, y_adas13_t)\n",
    "adas13_models[1].fit(X_train_scaled, y_adas13_tp1)\n",
    "adas13_models[2].fit(X_train_scaled, y_adas13_tp2)\n",
    "adas13_models[3].fit(X_train_scaled, y_adas13_tp3)\n",
    "adas13_models[4].fit(X_train_scaled, y_adas13_tp4)\n",
    "adas13_models[5].fit(X_train_scaled, y_adas13_tp5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making Prediction CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make prediction matrices\n",
    "predicted_diagnosis_test = []\n",
    "predicted_adas13_test = []\n",
    "predicted_mmse_test = []\n",
    "predicted_vn_test = []\n",
    "\n",
    "predicted_diagnosis_valid = []\n",
    "predicted_adas13_valid = []\n",
    "predicted_mmse_valid = []\n",
    "predicted_vn_valid = []\n",
    "for i in range(6):\n",
    "    predicted_diagnosis_test.append(diagnosis_models[i].predict(X_test_scaled))\n",
    "    predicted_adas13_test.append(adas13_models[i].predict(X_test_scaled))\n",
    "    predicted_mmse_test.append(mmse_models[i].predict(X_test_scaled))\n",
    "    predicted_vn_test.append(vn_models[i].predict(X_test_scaled))\n",
    "    \n",
    "    predicted_diagnosis_valid.append(diagnosis_models[i].predict(X_validation_scaled))\n",
    "    predicted_adas13_valid.append(adas13_models[i].predict(X_validation_scaled))\n",
    "    predicted_mmse_valid.append(mmse_models[i].predict(X_validation_scaled))\n",
    "    predicted_vn_valid.append(vn_models[i].predict(X_validation_scaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PTID indexes in the prediction matrices \n",
    "X_test_indices = list(Xts_test_df['PTID_Key'])\n",
    "X_valid_indices = list(Xts_target_df['PTID_Key']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get original dates of patient visits to create csv\n",
    "original_test_dates_df = pd.read_csv('TADPOLE_TargetData_test.csv')\n",
    "original_validation_dates_df = pd.read_csv('TADPOLE_PredictTargetData_valid.csv')\n",
    "\n",
    "#original_test_dates_df['Date'] = pd.to_datetime(original_test_dates_df['Date'],format=\"%Y-%m-%d\",errors='coerce')\n",
    "#original_validation_dates_df['Date'] = pd.to_datetime(original_validation_dates_df['Date'],format=\"%m/%d/%y\",errors='coerce')\n",
    "\n",
    "#keys to iterate over in the dictionaries in order\n",
    "original_test_order = []\n",
    "original_validation_order = []\n",
    "for index, row in original_test_dates_df.iterrows():\n",
    "    if row[1] not in original_test_order:\n",
    "        original_test_order.append(row[1])\n",
    "        \n",
    "for index, row in original_validation_dates_df.iterrows():\n",
    "    if row[1] not in original_validation_order:\n",
    "        original_validation_order.append(row[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "#indices to set results matrix\n",
    "ind_test = 0 \n",
    "ind_valid = 0\n",
    "\n",
    "#np matrices to hold results\n",
    "final_prediction_test = np.zeros( (original_test_dates_df.shape[0],7) )\n",
    "final_prediction_validation = np.zeros( (original_validation_dates_df.shape[0], 7) ) \n",
    "\n",
    "#iterate through keys in order they appear in original csv\n",
    "for key in original_test_order:\n",
    "    #get list of all timesteps to predict\n",
    "    timesteps_to_predict = test_ptid_prediction[key]\n",
    "    #the the corresponding index for the prediction matrix\n",
    "    corr_ptid_index = X_test_indices.index(key)\n",
    "    #print('PTID', key)\n",
    "    #print('corresponding index', corr_ptid_index)\n",
    "    for timestep in timesteps_to_predict:\n",
    "        #print('timestep', timestep)\n",
    "    \n",
    "        prediction_entry = np.zeros((7,))\n",
    "        #prediction_entry[0] = str(original_test_dates_df['Date'][ind_test])\n",
    "        prediction_entry[0] = key\n",
    "        prediction_entry[1:4] = classes_to_diagnosis(predicted_diagnosis_test[int(timestep)][corr_ptid_index])\n",
    "        prediction_entry[4] = predicted_adas13_test[int(timestep)][corr_ptid_index]\n",
    "        prediction_entry[5] = predicted_vn_test[int(timestep)][corr_ptid_index]\n",
    "        prediction_entry[6] = predicted_mmse_test[int(timestep)][corr_ptid_index]\n",
    "        \n",
    "        final_prediction_test[ind_test] = prediction_entry\n",
    "        ind_test += 1\n",
    "        \n",
    "\n",
    "#iterate through keys in order they appear in original csv\n",
    "for key in original_validation_order:\n",
    "    #get list of all timesteps to predict\n",
    "    timesteps_to_predict = target_ptid_prediction[key]\n",
    "    #the the corresponding index for the prediction matrix\n",
    "    corr_ptid_index = X_valid_indices.index(key)\n",
    "    for timestep in timesteps_to_predict:\n",
    "    \n",
    "        prediction_entry = np.zeros((7,))\n",
    "        #prediction_entry[0] = str(original_validation_dates_df['Date'][ind_valid])\n",
    "        prediction_entry[0] = key\n",
    "        prediction_entry[1:4] = classes_to_diagnosis(predicted_diagnosis_valid[int(timestep)][corr_ptid_index])\n",
    "        prediction_entry[4] = predicted_adas13_valid[int(timestep)][corr_ptid_index]\n",
    "        prediction_entry[5] = predicted_vn_valid[int(timestep)][corr_ptid_index]\n",
    "        prediction_entry[6] = predicted_mmse_valid[int(timestep)][corr_ptid_index]\n",
    "        \n",
    "        final_prediction_validation[ind_valid] = prediction_entry\n",
    "        ind_valid += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concatenating Dates w/ Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_test_data = np.column_stack( (original_test_dates_df.values[:,0], final_prediction_test) )\n",
    "final_validation_data = np.column_stack( (original_validation_dates_df.values[:,0], final_prediction_validation) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_test_data_df = pd.DataFrame(data=final_test_data, columns=original_test_dates_df.columns)\n",
    "final_validation_data_df = pd.DataFrame(data=final_validation_data, columns=original_validation_dates_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = 'results/'\n",
    "output_test = 'TADPOLE_test_predictions.csv'\n",
    "output_validation = 'TADPOLE_validation_predictions.csv'\n",
    "final_test_data_df.to_csv( (output_folder+output_test), index=False )\n",
    "final_validation_data_df.to_csv( (output_folder+output_validation), index=False )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(generated_csv, given_csv):\n",
    "    my_prediction = pd.read_csv(generated_csv)\n",
    "    ground_truth = pd.read_csv(given_csv)\n",
    "    valid_index = list()\n",
    "    \n",
    "    #get list of valid indices to compare\n",
    "    for index, row in ground_truth.iterrows():\n",
    "        diagnoses = row[2:5]\n",
    "        valid_entry = pd.isnull(diagnoses)\n",
    "        if(list(valid_entry).count(False)==len(diagnoses)):\n",
    "            valid_index.append(index)\n",
    "        \n",
    "    #iterate through valid indicies and compare classification error\n",
    "    num_instances = len(valid_index)\n",
    "    num_correct = 0\n",
    "    \n",
    "    for index in valid_index:\n",
    "        yhat = my_prediction.values[index]\n",
    "        y = ground_truth.values[index]\n",
    "        if( np.array_equal(yhat[2:5], y[2:5]) ):\n",
    "            num_correct+=1\n",
    "    return float(num_correct/num_instances)\n",
    "\n",
    "def calculate_mse(generated_csv, given_csv):\n",
    "    #load csvs\n",
    "    my_prediction = pd.read_csv(generated_csv)\n",
    "    ground_truth = pd.read_csv(given_csv)\n",
    "    \n",
    "    adas_samples = 0 #counts number of samples\n",
    "    adas_mse = 0 #accumulator variable for mse\n",
    "    adas_mae = 0 #accumulator variable for mae\n",
    "    \n",
    "    mmse_samples = 0\n",
    "    mmse_mse = 0\n",
    "    mmse_mae = 0\n",
    "    \n",
    "    vn_samples = 0\n",
    "    vn_mse = 0\n",
    "    vn_mae = 0\n",
    "    \n",
    "    for index, row in ground_truth.iterrows():\n",
    "        #if there is a value at that positions, calculate the squared difference and increment the samples\n",
    "        if not np.isnan(row[5]):\n",
    "            adas_mse+=(my_prediction.values[index][5] - row[5])**2\n",
    "            adas_samples+=1\n",
    "            adas_mae += abs(my_prediction.values[index][5] - row[5])\n",
    "        if not np.isnan(row[6]):\n",
    "            vn_mse += (my_prediction.values[index][6] - row[6])**2\n",
    "            vn_samples += 1\n",
    "            vn_mae += abs(my_prediction.values[index][6] - row[6])\n",
    "        if not np.isnan(row[7]):\n",
    "            mmse_mse += (my_prediction.values[index][7] - row[7])**2\n",
    "            mmse_samples += 1\n",
    "            mmse_mae += abs(my_prediction.values[index][7] - row[7])\n",
    "            \n",
    "    #divide by the number of respective samples\n",
    "    adas_mse /= adas_samples\n",
    "    vn_mse /= vn_samples\n",
    "    mmse_mse /= mmse_samples\n",
    "    \n",
    "    adas_mae /= adas_samples\n",
    "    vn_mae /= vn_samples\n",
    "    mmse_mae /= mmse_samples\n",
    "    \n",
    "    #create a list for returning\n",
    "    mse_list = [adas_mse, vn_mse, mmse_mse]\n",
    "    mae_list = [adas_mae, vn_mae, mmse_mae]\n",
    "    return mse_list, mae_list\n",
    "\n",
    "def calculate_class_accuracy(generated_csv, given_csv):\n",
    "    my_prediction = pd.read_csv(generated_csv)\n",
    "    ground_truth = pd.read_csv(given_csv)\n",
    "    \n",
    "    num_right = np.array([0,0,0])\n",
    "    total_num = np.array([0,0,0])\n",
    "    \n",
    "    \n",
    "    for index, row in ground_truth.iterrows():\n",
    "        diagnoses = row[2:5]\n",
    "        valid_entry = pd.isnull(diagnoses)\n",
    "        \n",
    "        #print(my_current_prediction)\n",
    "        if(list(valid_entry).count(False)==len(diagnoses)):\n",
    "            #print(diagnoses)\n",
    "            my_current_prediction = diagnosis_to_classes(my_prediction.values[index][2:5],1)\n",
    "            actual_prediction = diagnosis_to_classes(diagnoses,1)\n",
    "            #print(actual_prediction)\n",
    "            \n",
    "            if my_current_prediction == actual_prediction:                \n",
    "                num_right[int(actual_prediction)] += 1\n",
    "                total_num[int(actual_prediction)] += 1\n",
    "            else:\n",
    "                total_num[int(actual_prediction)] += 1\n",
    "            \n",
    "    return num_right/total_num\n",
    "        \n",
    "    \n",
    "            \n",
    "        \n",
    "        \n",
    "\n",
    "#def get_training_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.86936937  0.77922078  0.65517241]\n"
     ]
    }
   ],
   "source": [
    "results_dir = 'results/'\n",
    "ground_truth_test = 'TADPOLE_TargetData_test.csv'\n",
    "ground_truth_validation = 'TADPOLE_TargetData_valid.csv'\n",
    "\n",
    "baseline_test = 'TADPOLE_baseline_test.csv'\n",
    "baseline_validation = 'TADPOLE_baseline_valid.csv'\n",
    "\n",
    "generated_test = output_test\n",
    "generated_validation = output_validation\n",
    "print(calculate_class_accuracy((results_dir + generated_test), (results_dir + ground_truth_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir = 'results/'\n",
    "ground_truth_test = 'TADPOLE_TargetData_test.csv'\n",
    "ground_truth_validation = 'TADPOLE_TargetData_valid.csv'\n",
    "\n",
    "baseline_test = 'TADPOLE_baseline_test.csv'\n",
    "baseline_validation = 'TADPOLE_baseline_valid.csv'\n",
    "\n",
    "generated_test = output_test\n",
    "generated_validation = output_validation\n",
    "\n",
    "test_accuracy = get_accuracy( (results_dir + generated_test), (results_dir + ground_truth_test) )\n",
    "test_mse_loss, test_mae_loss = calculate_mse( (results_dir + generated_test), (results_dir + ground_truth_test) )\n",
    "test_class_accuracy = calculate_class_accuracy((results_dir + generated_test), (results_dir + ground_truth_test))\n",
    "\n",
    "validation_accuracy = get_accuracy( (results_dir + generated_validation), (results_dir + ground_truth_validation) )\n",
    "validation_mse_loss, validation_mae_loss = calculate_mse( (results_dir + generated_validation), (results_dir + ground_truth_validation) )\n",
    "validation_class_accuracy = calculate_class_accuracy((results_dir + generated_validation), (results_dir + ground_truth_validation))\n",
    "\n",
    "baseline_test_accuracy = get_accuracy( (results_dir + baseline_test), (results_dir + ground_truth_test) )\n",
    "baseline_test_mse_loss, baseline_test_mae_loss = calculate_mse( (results_dir + baseline_test), (results_dir + ground_truth_test) )\n",
    "baseline_test_class_accuracy = calculate_class_accuracy( (results_dir + baseline_test), (results_dir + ground_truth_test))\n",
    "\n",
    "baseline_validation_accuracy = get_accuracy( (results_dir + baseline_validation), (results_dir + ground_truth_validation) )\n",
    "baseline_validation_mse_loss, baseline_validation_mae_loss = calculate_mse( (results_dir + baseline_validation), (results_dir + ground_truth_validation) )\n",
    "baseline_validation_class_accuracy = calculate_class_accuracy((results_dir + baseline_validation), (results_dir + ground_truth_validation))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline test accuracy: 0.45694200351493847\n",
      "baseline test adas mse: 218.68641114982577\n",
      "baseline test vn mse: 0.0002852371448033057\n",
      "baseline test mmse mse: 20.697594501718214\n",
      "baseline test adas mae: 10.181184668989546\n",
      "baseline test vn mae: 0.011747535516014236\n",
      "baseline test mmse mae: 2.8831615120274914\n",
      "baseline test class accuracy [ 0.6981982   0.29004329  0.32758621]\n",
      "\n",
      "\n",
      "baseline validation accuracy: 0.3548387096774194\n",
      "baseline validation adas mse: 239.65290806754223\n",
      "baseline validation vn mse: 0.00022922046268185887\n",
      "baseline validation mmse mse: 33.26951672862454\n",
      "baseline validation adas mae: 10.868667917448406\n",
      "baseline validation vn mae: 0.011603050190476192\n",
      "baseline validation mmse mae: 3.4479553903345725\n",
      "baseline validation class accuracy [ 0.51428571  0.32089552  0.24369748]\n",
      "\n",
      "\n",
      "refined test accuracy: 0.789103690685413\n",
      "refined test adas mse: 72.53164693830362\n",
      "refined test vn mse: 0.00029722749494302483\n",
      "refined test mmse mse: 8.840569578948987\n",
      "refined test adas mae: 6.031887462290427\n",
      "refined test vn mae: 0.014684586120996436\n",
      "refined test mmse mae: 2.1485181293246844\n",
      "refined test class accuracy: [ 0.86936937  0.77922078  0.65517241]\n",
      "\n",
      "\n",
      "refined validation accuracy: 0.8292220113851992\n",
      "refined validation adas mse: 91.89935767682488\n",
      "refined validation vn mse: 0.0002995567457511358\n",
      "refined validation mmse mse: 10.549923805539192\n",
      "refined validation adas mae: 6.336127035721973\n",
      "refined validation vn mae: 0.015092037362637355\n",
      "refined validation mmse mae: 2.2207991104512197\n",
      "refined validation class accuracy: [ 0.81428571  0.9141791   0.65546218]\n",
      "\n",
      "\n",
      "increase in test accuracy: 0.3321616871704745\n",
      "increase in validation accuracy: 0.47438330170777987\n"
     ]
    }
   ],
   "source": [
    "print('baseline test accuracy:', baseline_test_accuracy)\n",
    "print('baseline test adas mse:', baseline_test_mse_loss[0])\n",
    "print('baseline test vn mse:', baseline_test_mse_loss[1])\n",
    "print('baseline test mmse mse:', baseline_test_mse_loss[2])\n",
    "print('baseline test adas mae:', baseline_test_mae_loss[0])\n",
    "print('baseline test vn mae:', baseline_test_mae_loss[1])\n",
    "print('baseline test mmse mae:', baseline_test_mae_loss[2])\n",
    "print('baseline test class accuracy', baseline_test_class_accuracy)\n",
    "print('\\n')\n",
    "print('baseline validation accuracy:', baseline_validation_accuracy)\n",
    "print('baseline validation adas mse:', baseline_validation_mse_loss[0])\n",
    "print('baseline validation vn mse:', baseline_validation_mse_loss[1])\n",
    "print('baseline validation mmse mse:', baseline_validation_mse_loss[2])\n",
    "print('baseline validation adas mae:', baseline_validation_mae_loss[0])\n",
    "print('baseline validation vn mae:', baseline_validation_mae_loss[1])\n",
    "print('baseline validation mmse mae:', baseline_validation_mae_loss[2])\n",
    "print('baseline validation class accuracy', baseline_validation_class_accuracy)\n",
    "print('\\n')\n",
    "print('refined test accuracy:', test_accuracy)\n",
    "print('refined test adas mse:', test_mse_loss[0])\n",
    "print('refined test vn mse:', test_mse_loss[1])\n",
    "print('refined test mmse mse:', test_mse_loss[2])\n",
    "print('refined test adas mae:', test_mae_loss[0])\n",
    "print('refined test vn mae:', test_mae_loss[1])\n",
    "print('refined test mmse mae:', test_mae_loss[2])\n",
    "print('refined test class accuracy:', test_class_accuracy)\n",
    "print('\\n')\n",
    "print('refined validation accuracy:', validation_accuracy)\n",
    "print('refined validation adas mse:', validation_mse_loss[0])\n",
    "print('refined validation vn mse:', validation_mse_loss[1])\n",
    "print('refined validation mmse mse:', validation_mse_loss[2])\n",
    "print('refined validation adas mae:', validation_mae_loss[0])\n",
    "print('refined validation vn mae:', validation_mae_loss[1])\n",
    "print('refined validation mmse mae:', validation_mae_loss[2])\n",
    "print('refined validation class accuracy:', validation_class_accuracy)\n",
    "print('\\n')\n",
    "print('increase in test accuracy:', test_accuracy-baseline_test_accuracy)\n",
    "print('increase in validation accuracy:', validation_accuracy-baseline_validation_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_overall_tpr(generated_csv, given_csv):\n",
    "    my_prediction = pd.read_csv(generated_csv)\n",
    "    ground_truth = pd.read_csv(given_csv)\n",
    "    \n",
    "    overall_tpr = []\n",
    "    #overall_fpr = []\n",
    "    overall = 0\n",
    "    total_samples = 0\n",
    "    #tpr_class = np.array([0,0,0])\n",
    "    #fpr_class = np.array([0,0,0])\n",
    "    \n",
    "    \n",
    "    \n",
    "    for index, row in ground_truth.iterrows():\n",
    "        diagnoses = row[2:5]\n",
    "        valid_entry = pd.isnull(diagnoses)\n",
    "        if(list(valid_entry).count(False)==len(diagnoses)):\n",
    "            #print(diagnoses)\n",
    "            total_samples += 1\n",
    "            my_current_prediction = diagnosis_to_classes(my_prediction.values[index][2:5],1)\n",
    "            actual_prediction = diagnosis_to_classes(diagnoses,1)\n",
    "            \n",
    "            if my_current_prediction == actual_prediction:\n",
    "                overall += 1\n",
    "                overall_tpr.append(overall/total_samples)\n",
    "                #tpr_class[int(actual_prediction)] += 1\n",
    "            \n",
    "    return overall_tpr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'overall_tpr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-271-7ec39d6a5c52>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtpr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_overall_tpr\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mresults_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mgenerated_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mresults_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mground_truth_test\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-270-47c91034f9ef>\u001b[0m in \u001b[0;36mget_overall_tpr\u001b[0;34m(generated_csv, given_csv)\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmy_current_prediction\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mactual_prediction\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m                 \u001b[0moverall\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m                 \u001b[0moverall_tpr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moverall\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mtotal_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m                 \u001b[0;31m#tpr_class[int(actual_prediction)] += 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'overall_tpr' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tpr = get_overall_tpr( (results_dir + generated_test), (results_dir + ground_truth_test) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

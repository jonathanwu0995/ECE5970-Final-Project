{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonathanwu/anaconda3/envs/ECE5970/lib/python3.5/site-packages/IPython/core/interactiveshell.py:2698: DtypeWarning: Columns (85,86,91,92,101,102,103,104,105,106,107,108,109,456,820,1398,1643,1645,1651,1652,1881,1882,1883,1884,1885,1886,1887,1888,1889,1890,1891) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Importing the dataset with library pandas\n",
    "dataset = pd.read_csv('TADPOLE_InputData.csv')\n",
    "labels_train = pd.read_csv('TADPOLE_TargetData_train.csv')\n",
    "labels_test = pd.read_csv('TADPOLE_TargetData_test.csv')\n",
    "target = pd.read_csv('TADPOLE_PredictTargetData_valid.csv')\n",
    "\n",
    "# Drop meaningless rows and columns. A good practice is to drop rows before columns.\n",
    "dataset = dataset[~np.isnan(dataset['PTID_Key'])] # Drop patients with no ID, since they cannot be used for learning or prediction. [] slices the rows in dataframe.\n",
    "dataset = dataset.dropna(axis=1, how='all') # Drop empty columns in dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These columns are time stamps that do not carry particular meaning, so dropped for now.\n",
    "badColumns = ['update_stamp_UCSFFSL_02_01_16_UCSFFSL51ALL_08_01_16',\n",
    "    'update_stamp_UCSFFSX_11_02_15_UCSFFSX51_08_01_16',\n",
    "    'update_stamp_UCBERKELEYAV45_10_17_16',\n",
    "    'update_stamp_DTIROI_04_30_14']\n",
    "dataset = dataset.drop(badColumns,axis=1) # Remove this section from objDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Unfortunately, some numerical columns contain non-numerical character such as '>' in  '>1300'.\n",
    "# The strategy is to convert these columns to floats by extracting only numbers. For example, '>1300' goes to 1300\n",
    "columnsObjToNum = ['ABETA_UPENNBIOMK9_04_19_17','TAU_UPENNBIOMK9_04_19_17','PTAU_UPENNBIOMK9_04_19_17','COMMENT_UPENNBIOMK9_04_19_17']\n",
    "for column in columnsObjToNum:\n",
    "    colIdx = dataset.columns.get_loc(column)\n",
    "    rowIdx = np.where(dataset[column].apply(type).values == str)[0] # Find all str type elements in each column, which may or may not contain non-numerical characters such as '<' or '>'.\n",
    "    for row in rowIdx: # iterate through each row of string type element in the column\n",
    "        dataset.iloc[row,colIdx] = float(re.sub(\"[^0-9.]\",\"\",dataset[column].values[row])) # Find the float/int number in the string, and cast to float type. \n",
    "dataset[columnsObjToNum] = dataset[columnsObjToNum].astype(float) # cast each column to float type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert date columns to date format in dataset, since they are currently imported as object columns\n",
    "for column in dataset: # variable 'column' is a string\n",
    "    if dataset[column].dtype == 'object' and dataset[column].str.match('[0-9]+/[0-9]+/[0-9]+').sum() > 0: # returns true if this column contains at least one string that matches date format.\n",
    "        dataset[column] = pd.to_datetime(dataset[column],format=\"%m/%d/%y\",errors='coerce') # convert string to date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some numerical columns contain only one/few possible values, which are more likely to be categorical than numerical features.\n",
    "# As a result, such columns are converted to objective dtype. e.g. some column with only -4 and nan.\n",
    "columnsNumToCat = []\n",
    "for column in dataset:\n",
    "    psbVal = dataset[column].unique()\n",
    "    if psbVal.dtype == 'float64' and psbVal.size <= 20:\n",
    "        columnsNumToCat.append(column)\n",
    "        validRowIdx = dataset[column].notnull()\n",
    "        dataset[column].loc[validRowIdx] = dataset[column].loc[validRowIdx].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tally the data types of all data columns, and then separate them according to dtype.\n",
    "dtypeCounts = dataset.dtypes.value_counts(); # Count the number of columns for each data type. Turns out to be only 'float64' and 'object'.\n",
    "numDataset = dataset.select_dtypes(include=['float'])\n",
    "objDataset = dataset.select_dtypes(include=['object'])\n",
    "dateDataset = dataset.select_dtypes(include=['datetime64']) # select dates from objDataset for variable dateDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputing missing data in numDataset\n",
    "from sklearn.preprocessing import Imputer\n",
    "imp = Imputer(missing_values = 'NaN', strategy = 'mean',axis = 0) # impute numerical columns\n",
    "imp = imp.fit(numDataset)\n",
    "numX = imp.transform(numDataset) # Extract data from numData as numX in dtype ndarray\n",
    "#numAttributes = numDataset.columns.values # Names of columns in numX. Executed after imputation since all NaN columns are dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonathanwu/anaconda3/envs/ECE5970/lib/python3.5/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "# Encoding categorical data\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "objDataset.loc[-1,:] = np.repeat(np.nan,objDataset.shape[1]) # Append one NaN to the end of each column so that NaN must be a class for each column.\n",
    "objDataset = objDataset.fillna(value=' ') # LabelEncoder does not work with NaN, so NaN is converted to a space ' ', which is always sorted as the first class by LabelEncoder.\n",
    "le = LabelEncoder()\n",
    "catClasses = np.array([]) # initiate an empty list of attribute names\n",
    "catAttributes = np.array([]) # initiate an empty list of attribute names\n",
    "for i in range(objDataset.shape[1]):\n",
    "    objDataset.iloc[:,i] = le.fit_transform(objDataset.iloc[:,i]) # encode column i\n",
    "    classes = le.classes_ # All the labels in column i, including ' ', which was translated from NaN\n",
    "    classes[0] = 'NaN' # Replace ' ' with the attribute of the column\n",
    "    catClasses = np.append(catClasses,classes) # Append column attribute followed by all its labels to catAttribute\n",
    "    catAttributes = np.append(catAttributes, np.repeat(objDataset.columns.values[i],classes.size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
